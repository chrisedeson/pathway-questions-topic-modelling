{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "4eb2ca6b",
      "metadata": {
        "id": "4eb2ca6b"
      },
      "source": [
        "# Hybrid Topic Discovery & Classification with AWS Integration\n",
        "\n",
        "**Purpose**: Classify questions against existing topics and discover new topics using hybrid approach.\n",
        "\n",
        "**Data Flow**:\n",
        "1. Load topics from Google Sheets\n",
        "2. Load student questions from Langfuse CSV\n",
        "3. Similarity classification (threshold-based)\n",
        "4. Clustering for new topic discovery\n",
        "5. Output parquet files to AWS S3\n",
        "\n",
        "**Key Features**:\n",
        "- AWS S3 for embeddings cache and outputs\n",
        "- Environment-responsive configuration\n",
        "- Comprehensive error logging\n",
        "- Analytics outputs for Streamlit dashboard"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3e89b193",
      "metadata": {
        "id": "3e89b193"
      },
      "source": [
        "## Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "id": "da8a424b",
      "metadata": {
        "id": "da8a424b"
      },
      "outputs": [],
      "source": [
        "!pip install -q openai pandas numpy scipy scikit-learn matplotlib seaborn tqdm umap-learn hdbscan bertopic backoff boto3 gspread oauth2client pyarrow fastparquet python-dotenv"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3dca8fb7",
      "metadata": {
        "id": "3dca8fb7"
      },
      "source": [
        "## Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "id": "1fa6d765",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1fa6d765",
        "outputId": "2aab1049-4ca8-4435-eeeb-743f7ce983b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Configuration loaded\n",
            "   Mode: all, Threshold: 0.7\n",
            "   S3 Bucket: byupathway-public\n",
            "   Embedding Model: text-embedding-3-small\n"
          ]
        }
      ],
      "source": [
        "# Processing settings\n",
        "EVAL_MODE = \"all\"  # \"sample\" or \"all\"\n",
        "SAMPLE_SIZE = 1000\n",
        "SIMILARITY_THRESHOLD = 0.70\n",
        "REPRESENTATIVE_QUESTION_METHOD = \"centroid\"  # \"centroid\" or \"frequent\"\n",
        "\n",
        "# Model settings\n",
        "EMBEDDING_MODEL = \"text-embedding-3-small\"\n",
        "EMBEDDING_DIMENSIONS = 1536\n",
        "GPT_MODEL = \"gpt-5-nano\"\n",
        "\n",
        "# AWS S3 settings\n",
        "S3_BUCKET = \"byupathway-public\"\n",
        "S3_OUTPUT_PREFIX = \"topic-modeling-data\"\n",
        "S3_CACHE_PREFIX = \"embeddings-cache\"\n",
        "S3_REGION = \"us-east-1\"\n",
        "\n",
        "# Embedding storage settings\n",
        "EMBEDDING_STORAGE = \"local\"  # \"s3\" or \"local\"\n",
        "LOCAL_CACHE_DIR = \"./embedding_cache\"  # Directory for local embedding storage\n",
        "\n",
        "# Clustering settings\n",
        "UMAP_N_COMPONENTS = 5\n",
        "HDBSCAN_MIN_CLUSTER_SIZE = 3\n",
        "RANDOM_SEED = 42\n",
        "\n",
        "# Google Sheets URL (This should come from Elder Edwards)\n",
        "GOOGLE_SHEETS_URL = \"https://docs.google.com/spreadsheets/d/1L3kOmE6mZEVotjUu2ZuLqir2rZ4c0yfNeeTuHwf3JQI/edit?gid=0#gid=0\"\n",
        "\n",
        "print(\"‚úÖ Configuration loaded\")\n",
        "print(f\"   Mode: {EVAL_MODE}, Threshold: {SIMILARITY_THRESHOLD}\")\n",
        "print(f\"   S3 Bucket: {S3_BUCKET}\")\n",
        "print(f\"   Embedding Model: {EMBEDDING_MODEL}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2988c527",
      "metadata": {
        "id": "2988c527"
      },
      "source": [
        "## Environment Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "id": "e89070b2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e89070b2",
        "outputId": "8d3e2787-c25f-48a1-fcb7-bb648e99315b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîß Running in Google Colab\n",
            "‚úÖ Environment setup complete\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy.spatial.distance import cosine\n",
        "import json\n",
        "import pickle\n",
        "from pathlib import Path\n",
        "import time\n",
        "from datetime import datetime\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from typing import List, Dict, Tuple, Optional, Any\n",
        "import asyncio\n",
        "import backoff\n",
        "import re\n",
        "import hashlib\n",
        "import boto3\n",
        "from botocore.exceptions import ClientError\n",
        "import gspread\n",
        "from oauth2client.service_account import ServiceAccountCredentials\n",
        "\n",
        "# Detect environment\n",
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "    from google.colab import userdata\n",
        "    print(\"üîß Running in Google Colab\")\n",
        "except ImportError:\n",
        "    IN_COLAB = False\n",
        "    from dotenv import load_dotenv\n",
        "    load_dotenv()\n",
        "    print(\"üîß Running locally\")\n",
        "\n",
        "# Load credentials\n",
        "if IN_COLAB:\n",
        "    OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n",
        "    AWS_ACCESS_KEY = userdata.get('AWS_ACCESS_KEY_ID')\n",
        "    AWS_SECRET_KEY = userdata.get('AWS_SECRET_ACCESS_KEY')\n",
        "    GOOGLE_SERVICE_ACCOUNT = userdata.get('GOOGLE_SERVICE_ACCOUNT_JSON')\n",
        "else:\n",
        "    OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
        "    AWS_ACCESS_KEY = os.getenv('AWS_ACCESS_KEY_ID')\n",
        "    AWS_SECRET_KEY = os.getenv('AWS_SECRET_ACCESS_KEY')\n",
        "    GOOGLE_SERVICE_ACCOUNT = os.getenv('GOOGLE_SERVICE_ACCOUNT_JSON')\n",
        "\n",
        "# Initialize OpenAI\n",
        "from openai import OpenAI, AsyncOpenAI\n",
        "client = OpenAI(api_key=OPENAI_API_KEY)\n",
        "async_client = AsyncOpenAI(api_key=OPENAI_API_KEY)\n",
        "\n",
        "# Initialize AWS S3\n",
        "s3_client = boto3.client(\n",
        "    's3',\n",
        "    aws_access_key_id=AWS_ACCESS_KEY,\n",
        "    aws_secret_access_key=AWS_SECRET_KEY,\n",
        "    region_name=S3_REGION\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Environment setup complete\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6199ff1c",
      "metadata": {
        "id": "6199ff1c"
      },
      "source": [
        "## Error Logger"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "id": "09f9701f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "09f9701f",
        "outputId": "74bceb63-eaff-48d2-c2e6-678d04f64281"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Error logger initialized\n"
          ]
        }
      ],
      "source": [
        "class ErrorLogger:\n",
        "    def __init__(self):\n",
        "        self.errors = []\n",
        "        self.warnings = []\n",
        "        self.rows_dropped = []\n",
        "        self.timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "\n",
        "    def log_error(self, stage: str, message: str, data: Any = None):\n",
        "        entry = {\"timestamp\": datetime.now().isoformat(), \"stage\": stage, \"message\": message, \"data\": str(data)}\n",
        "        self.errors.append(entry)\n",
        "        print(f\"‚ùå ERROR [{stage}]: {message}\")\n",
        "\n",
        "    def log_warning(self, stage: str, message: str, data: Any = None):\n",
        "        entry = {\"timestamp\": datetime.now().isoformat(), \"stage\": stage, \"message\": message, \"data\": str(data)}\n",
        "        self.warnings.append(entry)\n",
        "        print(f\"‚ö†Ô∏è  WARNING [{stage}]: {message}\")\n",
        "\n",
        "    def log_dropped_row(self, stage: str, reason: str, row_data: Any):\n",
        "        entry = {\"timestamp\": datetime.now().isoformat(), \"stage\": stage, \"reason\": reason, \"row_data\": str(row_data)}\n",
        "        self.rows_dropped.append(entry)\n",
        "\n",
        "    def get_summary(self):\n",
        "        return {\n",
        "            \"total_errors\": len(self.errors),\n",
        "            \"total_warnings\": len(self.warnings),\n",
        "            \"total_dropped_rows\": len(self.rows_dropped),\n",
        "            \"errors\": self.errors,\n",
        "            \"warnings\": self.warnings,\n",
        "            \"dropped_rows\": self.rows_dropped\n",
        "        }\n",
        "\n",
        "    def save_to_file(self, filename: str):\n",
        "        with open(filename, 'w') as f:\n",
        "            json.dump(self.get_summary(), f, indent=2)\n",
        "        print(f\"üìù Error log saved: {filename}\")\n",
        "        return filename\n",
        "\n",
        "error_logger = ErrorLogger()\n",
        "print(\"‚úÖ Error logger initialized\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e40fc8e3",
      "metadata": {
        "id": "e40fc8e3"
      },
      "source": [
        "## AWS S3 Utilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "id": "2e3d272b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2e3d272b",
        "outputId": "60aedf62-f664-4e89-c453-6149466b9c56"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ AWS S3 utilities ready (with retry logic)\n"
          ]
        }
      ],
      "source": [
        "@backoff.on_exception(\n",
        "    backoff.expo,\n",
        "    Exception,\n",
        "    max_tries=5,\n",
        "    max_time=30,\n",
        "    giveup=lambda e: isinstance(e, (KeyboardInterrupt, SystemExit))\n",
        ")\n",
        "def upload_to_s3(local_path: str, s3_key: str, public: bool = True) -> bool:\n",
        "    \"\"\"Upload file to S3 with retry logic and exponential backoff\n",
        "\n",
        "    Args:\n",
        "        local_path: Local file path to upload\n",
        "        s3_key: S3 key (path in bucket)\n",
        "        public: Whether to set public-read ACL (default True)\n",
        "\n",
        "    Returns:\n",
        "        bool: True if successful, False otherwise\n",
        "    \"\"\"\n",
        "    try:\n",
        "        extra_args = {'ACL': 'public-read'} if public else {}\n",
        "        s3_client.upload_file(local_path, S3_BUCKET, s3_key, ExtraArgs=extra_args)\n",
        "\n",
        "        if public:\n",
        "            url = f\"https://{S3_BUCKET}.s3.amazonaws.com/{s3_key}\"\n",
        "            print(f\"‚úÖ Uploaded to S3: {url}\")\n",
        "        else:\n",
        "            print(f\"‚úÖ Uploaded to S3: s3://{S3_BUCKET}/{s3_key}\")\n",
        "\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        error_logger.log_error(\"S3_Upload\", f\"Failed to upload {local_path} after retries\", e)\n",
        "        return False\n",
        "\n",
        "@backoff.on_exception(\n",
        "    backoff.expo,\n",
        "    Exception,\n",
        "    max_tries=3,\n",
        "    max_time=15,\n",
        "    giveup=lambda e: isinstance(e, (KeyboardInterrupt, SystemExit))\n",
        ")\n",
        "def download_from_s3(s3_key: str, local_path: str) -> bool:\n",
        "    \"\"\"Download file from S3 with retry logic\"\"\"\n",
        "    try:\n",
        "        s3_client.download_file(S3_BUCKET, s3_key, local_path)\n",
        "        return True\n",
        "    except ClientError as e:\n",
        "        if e.response['Error']['Code'] == '404':\n",
        "            # File doesn't exist - don't retry\n",
        "            return False\n",
        "        error_logger.log_error(\"S3_Download\", f\"Failed to download {s3_key} after retries\", e)\n",
        "        return False\n",
        "\n",
        "def delete_s3_folder(prefix: str):\n",
        "    \"\"\"Delete all objects with given prefix\"\"\"\n",
        "    try:\n",
        "        response = s3_client.list_objects_v2(Bucket=S3_BUCKET, Prefix=prefix)\n",
        "        if 'Contents' in response:\n",
        "            objects = [{'Key': obj['Key']} for obj in response['Contents']]\n",
        "            s3_client.delete_objects(Bucket=S3_BUCKET, Delete={'Objects': objects})\n",
        "            print(f\"üóëÔ∏è  Deleted {len(objects)} objects from s3://{S3_BUCKET}/{prefix}\")\n",
        "    except Exception as e:\n",
        "        error_logger.log_error(\"S3_Delete\", f\"Failed to delete folder {prefix}\", e)\n",
        "\n",
        "print(\"‚úÖ AWS S3 utilities ready (with retry logic)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "mzynRnU-5AVW",
      "metadata": {
        "id": "mzynRnU-5AVW"
      },
      "source": [
        "# Master File Sync System"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "id": "E65Qd05t5FWu",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E65Qd05t5FWu",
        "outputId": "0ccbfea6-55c4-4db1-9a7b-603568a021cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Master file sync system ready (S3-only)\n",
            "   Storage: s3://byupathway-public/langfuse/langfuse_traces_master.parquet\n"
          ]
        }
      ],
      "source": [
        "# S3 Master file settings\n",
        "S3_MASTER_KEY = \"langfuse/langfuse_traces_master.parquet\"\n",
        "\n",
        "def sync_master_file(new_data_df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Sync master file with S3 storage\n",
        "\n",
        "    Flow:\n",
        "    1. Download master file from S3 (if exists)\n",
        "    2. Merge new data with master\n",
        "    3. Remove duplicates (question/input + timestamp)\n",
        "    4. Upload updated master back to S3\n",
        "\n",
        "    Args:\n",
        "        new_data_df: New data to merge with master\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: Complete master dataframe\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"\\nüîÑ Syncing master file with S3...\")\n",
        "\n",
        "    local_master_path = \"/tmp/langfuse_traces_master.parquet\"\n",
        "    master_df = None\n",
        "\n",
        "    # Step 1: Try to load from S3\n",
        "    print(\"üìÇ Checking S3 for master file...\")\n",
        "    if download_from_s3(S3_MASTER_KEY, local_master_path):\n",
        "        try:\n",
        "            master_df = pd.read_parquet(local_master_path)\n",
        "            print(f\"‚úÖ Loaded master from S3: {len(master_df)} rows\")\n",
        "            print(f\"   Timestamp range: {master_df['timestamp'].min()} to {master_df['timestamp'].max()}\")\n",
        "        except Exception as e:\n",
        "            error_logger.log_error(\"MasterSync\", \"Failed to read master from S3\", e)\n",
        "            master_df = None\n",
        "\n",
        "    # Step 2: Merge new data with master\n",
        "    if master_df is None:\n",
        "        print(\"üìù No existing master file found - creating new one\")\n",
        "        merged_df = new_data_df.copy()\n",
        "    else:\n",
        "        print(f\"üîÄ Merging new data ({len(new_data_df)} rows) with master ({len(master_df)} rows)...\")\n",
        "\n",
        "        # Ensure both have same columns\n",
        "        for col in new_data_df.columns:\n",
        "            if col not in master_df.columns:\n",
        "                master_df[col] = None\n",
        "        for col in master_df.columns:\n",
        "            if col not in new_data_df.columns:\n",
        "                new_data_df[col] = None\n",
        "\n",
        "        # Combine\n",
        "        merged_df = pd.concat([master_df, new_data_df], ignore_index=True)\n",
        "        print(f\"   Combined: {len(merged_df)} rows\")\n",
        "\n",
        "    # Step 3: Remove duplicates (question/input + timestamp REQUIRED, id optional)\n",
        "    print(\"üßπ Removing duplicates (same question + timestamp)...\")\n",
        "    before_dedup = len(merged_df)\n",
        "\n",
        "    # Determine question column name (could be 'question' or 'input')\n",
        "    question_col = 'question' if 'question' in merged_df.columns else 'input' if 'input' in merged_df.columns else None\n",
        "\n",
        "    # Check for required columns\n",
        "    if question_col is None or 'timestamp' not in merged_df.columns:\n",
        "        print(\"   ‚ö†Ô∏è  Warning: Missing required columns (question/input and timestamp) for deduplication\")\n",
        "    else:\n",
        "        # Deduplicate by question/input + timestamp only\n",
        "        merged_df = merged_df.drop_duplicates(subset=[question_col, 'timestamp'], keep='first')\n",
        "\n",
        "        after_dedup = len(merged_df)\n",
        "        print(f\"   Removed {before_dedup - after_dedup} duplicates\")\n",
        "        print(f\"   Deduplication criteria: {question_col} + timestamp\")\n",
        "        print(f\"   Final: {len(merged_df)} rows\")\n",
        "\n",
        "    # Sort by timestamp (newest first)\n",
        "    if 'timestamp' in merged_df.columns:\n",
        "        merged_df = merged_df.sort_values('timestamp', ascending=False).reset_index(drop=True)\n",
        "\n",
        "    # Step 4: Upload to S3\n",
        "    print(\"‚òÅÔ∏è  Uploading to S3...\")\n",
        "    merged_df.to_parquet(local_master_path)\n",
        "    s3_success = upload_to_s3(local_master_path, S3_MASTER_KEY, public=False)\n",
        "\n",
        "    # Clean up temp file\n",
        "    try:\n",
        "        os.unlink(local_master_path)\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    if s3_success:\n",
        "        print(f\"‚úÖ Master file synced: {len(merged_df)} total rows\")\n",
        "    else:\n",
        "        print(f\"‚ùå S3 upload failed - master file not backed up!\")\n",
        "        error_logger.log_error(\"MasterSync\", \"Failed to upload master file to S3\", None)\n",
        "\n",
        "    return merged_df\n",
        "\n",
        "print(\"‚úÖ Master file sync system ready (S3-only)\")\n",
        "print(f\"   Storage: s3://{S3_BUCKET}/{S3_MASTER_KEY}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f4f833c4",
      "metadata": {
        "id": "f4f833c4"
      },
      "source": [
        "## Google Sheets Integration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "id": "6ded2f35",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ded2f35",
        "outputId": "cc336be8-4886-47dc-89f3-eff421cda462"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Google Sheets integration ready\n"
          ]
        }
      ],
      "source": [
        "def read_topics_from_google_sheets(sheet_url: str) -> pd.DataFrame:\n",
        "    \"\"\"Read topics from Google Sheets with flexible column handling\"\"\"\n",
        "    try:\n",
        "        creds_dict = json.loads(GOOGLE_SERVICE_ACCOUNT)\n",
        "        scope = ['https://spreadsheets.google.com/feeds', 'https://www.googleapis.com/auth/drive']\n",
        "        creds = ServiceAccountCredentials.from_json_keyfile_dict(creds_dict, scope)\n",
        "        gc = gspread.authorize(creds)\n",
        "\n",
        "        sheet = gc.open_by_url(sheet_url)\n",
        "        worksheet = sheet.get_worksheet(0)\n",
        "        data = worksheet.get_all_records()\n",
        "        df = pd.DataFrame(data)\n",
        "\n",
        "        # Handle both uppercase and lowercase column names\n",
        "        column_mapping = {}\n",
        "        for col in df.columns:\n",
        "            col_lower = col.lower()\n",
        "            if col_lower == 'topics' or col_lower == 'topic':\n",
        "                column_mapping[col] = 'topic'\n",
        "            elif col_lower == 'subtopics' or col_lower == 'subtopic':\n",
        "                column_mapping[col] = 'subtopic'\n",
        "            elif col_lower == 'questions' or col_lower == 'question':\n",
        "                column_mapping[col] = 'question'\n",
        "\n",
        "        df = df.rename(columns=column_mapping)\n",
        "\n",
        "        required = ['topic', 'subtopic', 'question']\n",
        "        if not all(col in df.columns for col in required):\n",
        "            raise ValueError(f\"Missing required columns. Found: {list(df.columns)}\")\n",
        "\n",
        "        df = df[required].dropna()\n",
        "        print(f\"‚úÖ Loaded {len(df)} topics from Google Sheets\")\n",
        "        print(f\"   Unique topics: {df['topic'].nunique()}, Unique subtopics: {df['subtopic'].nunique()}\")\n",
        "        return df\n",
        "\n",
        "    except Exception as e:\n",
        "        error_logger.log_error(\"GoogleSheets\", \"Failed to read topics\", e)\n",
        "        raise\n",
        "\n",
        "print(\"‚úÖ Google Sheets integration ready\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "01c2f81d",
      "metadata": {
        "id": "01c2f81d"
      },
      "source": [
        "## Langfuse Data Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "id": "41c220e2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "41c220e2",
        "outputId": "50f8cd41-4f8f-4fff-a9e0-9fe2a43a234f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Malformed row filter and ACM detector ready\n",
            "‚úÖ Langfuse cleaning utilities ready\n"
          ]
        }
      ],
      "source": [
        "def filter_malformed_rows(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Filter out malformed rows containing both 'kwargs' AND 'args' (case-insensitive)\n",
        "    This is the FIRST step in data cleaning to remove Langfuse error rows\n",
        "    \"\"\"\n",
        "    print(f\"üîç Filtering malformed rows (containing 'kwargs' AND 'args')...\")\n",
        "    before_filter = len(df)\n",
        "\n",
        "    # Convert all columns to string and check for both kwargs and args (case-insensitive)\n",
        "    df_str = df.astype(str)\n",
        "\n",
        "    # Check each row for both terms\n",
        "    malformed_mask = df_str.apply(\n",
        "        lambda row: any('kwargs' in str(val).lower() for val in row) and\n",
        "                   any('args' in str(val).lower() for val in row),\n",
        "        axis=1\n",
        "    )\n",
        "\n",
        "    # Keep only rows that are NOT malformed\n",
        "    filtered_df = df[~malformed_mask].copy()\n",
        "\n",
        "    after_filter = len(filtered_df)\n",
        "    filtered_count = before_filter - after_filter\n",
        "\n",
        "    print(f\"‚úÖ Filtered {filtered_count} malformed rows ({filtered_count/before_filter*100:.1f}%)\")\n",
        "    print(f\"   Remaining: {after_filter} rows\")\n",
        "\n",
        "    # Log dropped rows\n",
        "    for idx in df[malformed_mask].index:\n",
        "        error_logger.log_dropped_row(\n",
        "            \"MalformedFilter\",\n",
        "            \"Row contains both 'kwargs' and 'args' - likely Langfuse error\",\n",
        "            {\"index\": idx}\n",
        "        )\n",
        "\n",
        "    return filtered_df\n",
        "\n",
        "def detect_acm_question(question: str) -> bool:\n",
        "    \"\"\"Detect if question has ACM prefix\"\"\"\n",
        "    if not isinstance(question, str):\n",
        "        return False\n",
        "\n",
        "    pattern = r'^\\s*\\(ACMs?\\s+[Qq]uestion\\)\\s*:?\\s*'\n",
        "    return bool(re.search(pattern, question, flags=re.IGNORECASE))\n",
        "\n",
        "print(\"‚úÖ Malformed row filter and ACM detector ready\")\n",
        "\n",
        "\n",
        "def clean_langfuse_data(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Clean and validate Langfuse data with comprehensive error handling\"\"\"\n",
        "\n",
        "    print(f\"üßπ Cleaning {len(df)} Langfuse rows...\")\n",
        "    cleaned_rows = []\n",
        "\n",
        "    for idx, row in enumerate(df.itertuples(index=False), 1):\n",
        "        try:\n",
        "            cleaned_row = {}\n",
        "\n",
        "            # 0. Extract id (needed for deduplication)\n",
        "            id_value = getattr(row, 'id', None)\n",
        "            cleaned_row['id'] = id_value if pd.notna(id_value) else None\n",
        "\n",
        "            # 1. Extract timestamp\n",
        "            timestamp_value = getattr(row, 'timestamp', None)\n",
        "            if pd.notna(timestamp_value):\n",
        "                try:\n",
        "                    # Handle different timestamp formats\n",
        "                    if isinstance(timestamp_value, str):\n",
        "                        # Try parsing as ISO format first\n",
        "                        cleaned_row['timestamp'] = pd.to_datetime(timestamp_value).isoformat()\n",
        "                    else:\n",
        "                        cleaned_row['timestamp'] = pd.to_datetime(timestamp_value).isoformat()\n",
        "                except (ValueError, TypeError):\n",
        "                    # If parsing fails, try to extract from kwargs\n",
        "                    if hasattr(row, 'kwargs') and pd.notna(getattr(row, 'kwargs', None)):\n",
        "                        kwargs_str = str(getattr(row, 'kwargs', ''))\n",
        "                        # Look for timestamp in kwargs\n",
        "                        ts_match = re.search(r'\"timestamp\"\\s*:\\s*\"([^\"]*)\"', kwargs_str)\n",
        "                        if ts_match:\n",
        "                            try:\n",
        "                                cleaned_row['timestamp'] = pd.to_datetime(ts_match.group(1)).isoformat()\n",
        "                            except:\n",
        "                                cleaned_row['timestamp'] = None\n",
        "                        else:\n",
        "                            cleaned_row['timestamp'] = None\n",
        "                    else:\n",
        "                        cleaned_row['timestamp'] = None\n",
        "            else:\n",
        "                cleaned_row['timestamp'] = None\n",
        "\n",
        "            # 2. Extract input (malformed rows already filtered out)\n",
        "            input_value = getattr(row, 'input', None)\n",
        "\n",
        "            # Validate input is not empty\n",
        "            if pd.isna(input_value) or str(input_value).strip() == '':\n",
        "                error_logger.log_dropped_row(\"LangfuseClean\", \"Missing input\", {\"index\": idx})\n",
        "                continue\n",
        "\n",
        "            cleaned_row['input'] = str(input_value).strip()\n",
        "\n",
        "            # Detect if this is an ACM question (before cleaning the prefix)\n",
        "            cleaned_row['is_acm_question'] = detect_acm_question(cleaned_row['input'])\n",
        "\n",
        "            # 3. Extract output (no kwargs handling needed)\n",
        "            output_value = getattr(row, 'output', None)\n",
        "            cleaned_row['output'] = output_value if pd.notna(output_value) else None\n",
        "\n",
        "            # 4. Extract user_feedback\n",
        "            cleaned_row['user_feedback'] = getattr(row, 'user_feedback', None)\n",
        "\n",
        "            # 5. Parse metadata JSON for country, state, city, ip, language, is_suspicious\n",
        "            metadata_value = getattr(row, 'metadata', None)\n",
        "            if pd.notna(metadata_value) and metadata_value != '':\n",
        "                try:\n",
        "                    metadata = json.loads(metadata_value) if isinstance(metadata_value, str) else metadata_value\n",
        "\n",
        "                    # Extract geographic data\n",
        "                    cleaned_row['country'] = metadata.get('country')\n",
        "                    cleaned_row['state'] = metadata.get('state')\n",
        "                    cleaned_row['city'] = metadata.get('city')\n",
        "                    cleaned_row['ip_address'] = metadata.get('ip')\n",
        "                    cleaned_row['user_language'] = metadata.get('user_language')\n",
        "\n",
        "                    # Extract is_suspicious from security_validation\n",
        "                    security_val = metadata.get('security_validation', {})\n",
        "                    if isinstance(security_val, dict):\n",
        "                        cleaned_row['is_suspicious'] = security_val.get('is_suspicious', False)\n",
        "                    else:\n",
        "                        cleaned_row['is_suspicious'] = False\n",
        "\n",
        "                except (json.JSONDecodeError, TypeError, AttributeError) as e:\n",
        "                    # Metadata parsing failed, set nulls\n",
        "                    error_logger.log_error(\"LangfuseClean\", f\"Failed to parse metadata at row {idx}\", e)\n",
        "                    cleaned_row['country'] = None\n",
        "                    cleaned_row['state'] = None\n",
        "                    cleaned_row['city'] = None\n",
        "                    cleaned_row['ip_address'] = None\n",
        "                    cleaned_row['user_language'] = None\n",
        "                    cleaned_row['is_suspicious'] = False\n",
        "            else:\n",
        "                # No metadata, set nulls\n",
        "                cleaned_row['country'] = None\n",
        "                cleaned_row['state'] = None\n",
        "                cleaned_row['city'] = None\n",
        "                cleaned_row['ip_address'] = None\n",
        "                cleaned_row['user_language'] = None\n",
        "                cleaned_row['is_suspicious'] = False\n",
        "\n",
        "            cleaned_rows.append(cleaned_row)\n",
        "\n",
        "        except Exception as e:\n",
        "            error_logger.log_dropped_row(\"LangfuseClean\", f\"Unexpected error: {e}\", {\"index\": idx})\n",
        "            continue\n",
        "\n",
        "    cleaned_df = pd.DataFrame(cleaned_rows)\n",
        "\n",
        "    # Remove duplicates (same timestamp and input)\n",
        "    if 'timestamp' in cleaned_df.columns and 'input' in cleaned_df.columns:\n",
        "        before_dedup = len(cleaned_df)\n",
        "        cleaned_df = cleaned_df.drop_duplicates(subset=['timestamp', 'input'], keep='first')\n",
        "        after_dedup = len(cleaned_df)\n",
        "        if before_dedup > after_dedup:\n",
        "            print(f\"üóëÔ∏è  Removed {before_dedup - after_dedup} duplicate rows (same timestamp + input)\")\n",
        "\n",
        "    # Clean question prefixes (ACM Question)\n",
        "    cleaned_df['input'] = cleaned_df['input'].apply(lambda x: clean_question(x) if pd.notna(x) else x)\n",
        "\n",
        "    print(f\"‚úÖ Cleaned data: {len(cleaned_df)} rows ({len(df) - len(cleaned_df)} dropped)\")\n",
        "    print(f\"   Columns: {list(cleaned_df.columns)}\")\n",
        "    print(f\"   Country data: {cleaned_df['country'].notna().sum()} rows with country info\")\n",
        "\n",
        "    return cleaned_df\n",
        "\n",
        "print(\"‚úÖ Langfuse cleaning utilities ready\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5469ad7d",
      "metadata": {
        "id": "5469ad7d"
      },
      "source": [
        "## Question Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "id": "44c50dc2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "44c50dc2",
        "outputId": "83b88df6-956c-478f-9623-079d10e5ef46"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Question preprocessing ready\n"
          ]
        }
      ],
      "source": [
        "def clean_question(question: str) -> str:\n",
        "    \"\"\"Remove ACM prefixes and clean text\"\"\"\n",
        "    if not isinstance(question, str):\n",
        "        return str(question) if question is not None else \"\"\n",
        "\n",
        "    pattern = r'^\\s*\\(ACMs?\\s+[Qq]uestion\\)\\s*:?\\s*'\n",
        "    cleaned = re.sub(pattern, '', question, flags=re.IGNORECASE).strip()\n",
        "    return cleaned if cleaned else question\n",
        "\n",
        "def preprocess_dataframe(df: pd.DataFrame, question_col: str) -> pd.DataFrame:\n",
        "    \"\"\"Apply cleaning to question column\"\"\"\n",
        "    df = df.copy()\n",
        "    df[question_col] = df[question_col].apply(clean_question)\n",
        "    return df\n",
        "\n",
        "print(\"‚úÖ Question preprocessing ready\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0c2fddd4",
      "metadata": {
        "id": "0c2fddd4"
      },
      "source": [
        "## S3 Embeddings Cache"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "id": "ccae5ebc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ccae5ebc",
        "outputId": "1fd35d5f-5fd3-4feb-c864-f94d0ebcfcd6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Embeddings cache ready (with retry logic)\n",
            "   Storage: local\n"
          ]
        }
      ],
      "source": [
        "def get_cache_key(text: str, model: str) -> str:\n",
        "    \"\"\"Generate S3 cache key for text\"\"\"\n",
        "    text_hash = hashlib.md5(text.encode()).hexdigest()[:12]\n",
        "    return f\"{S3_CACHE_PREFIX}/{model}/{text_hash}.pkl\"\n",
        "\n",
        "def load_embedding_from_s3(text: str, model: str) -> Optional[List[float]]:\n",
        "    \"\"\"Load cached embedding from S3\"\"\"\n",
        "    cache_key = get_cache_key(text, model)\n",
        "    local_path = f\"/tmp/{cache_key.split('/')[-1]}\"\n",
        "\n",
        "    if download_from_s3(cache_key, local_path):\n",
        "        try:\n",
        "            with open(local_path, 'rb') as f:\n",
        "                embedding = pickle.load(f)\n",
        "            # Clean up temp file after reading\n",
        "            try:\n",
        "                os.unlink(local_path)\n",
        "            except:\n",
        "                pass\n",
        "            return embedding\n",
        "        except Exception as e:\n",
        "            # Clean up corrupted cache file\n",
        "            try:\n",
        "                os.unlink(local_path)\n",
        "            except:\n",
        "                pass\n",
        "    return None\n",
        "\n",
        "def save_embedding_to_s3(text: str, model: str, embedding: List[float]):\n",
        "    \"\"\"Save embedding to S3 cache with retry logic\"\"\"\n",
        "    cache_key = get_cache_key(text, model)\n",
        "    local_path = f\"/tmp/{cache_key.split('/')[-1]}\"\n",
        "\n",
        "    try:\n",
        "        # Write to local temp file\n",
        "        with open(local_path, 'wb') as f:\n",
        "            pickle.dump(embedding, f)\n",
        "\n",
        "        # Upload to S3 (with retry logic from upload_to_s3)\n",
        "        # Use public=False for cache files (no need for public access)\n",
        "        success = upload_to_s3(local_path, cache_key, public=False)\n",
        "\n",
        "        # Clean up temp file\n",
        "        try:\n",
        "            os.unlink(local_path)\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        return success\n",
        "    except Exception as e:\n",
        "        # Clean up on failure\n",
        "        try:\n",
        "            if os.path.exists(local_path):\n",
        "                os.unlink(local_path)\n",
        "        except:\n",
        "            pass\n",
        "        # Don't log - cache failures are expected and handled gracefully\n",
        "        return False\n",
        "\n",
        "def get_local_cache_path(text: str, model: str) -> str:\n",
        "    \"\"\"Generate local cache path for text\"\"\"\n",
        "    text_hash = hashlib.md5(text.encode()).hexdigest()[:12]\n",
        "    os.makedirs(LOCAL_CACHE_DIR, exist_ok=True)\n",
        "    return f\"{LOCAL_CACHE_DIR}/{model}_{text_hash}.pkl\"\n",
        "\n",
        "def load_embedding_from_local(text: str, model: str) -> Optional[List[float]]:\n",
        "    \"\"\"Load cached embedding from local storage\"\"\"\n",
        "    cache_path = get_local_cache_path(text, model)\n",
        "\n",
        "    if os.path.exists(cache_path):\n",
        "        try:\n",
        "            with open(cache_path, 'rb') as f:\n",
        "                embedding = pickle.load(f)\n",
        "            return embedding\n",
        "        except Exception as e:\n",
        "            # Clean up corrupted cache file\n",
        "            try:\n",
        "                os.unlink(cache_path)\n",
        "            except:\n",
        "                pass\n",
        "    return None\n",
        "\n",
        "def save_embedding_to_local(text: str, model: str, embedding: List[float]):\n",
        "    \"\"\"Save embedding to local cache\"\"\"\n",
        "    cache_path = get_local_cache_path(text, model)\n",
        "\n",
        "    try:\n",
        "        with open(cache_path, 'wb') as f:\n",
        "            pickle.dump(embedding, f)\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        return False\n",
        "\n",
        "def load_embedding(text: str, model: str) -> Optional[List[float]]:\n",
        "    \"\"\"Load cached embedding based on storage setting\"\"\"\n",
        "    if EMBEDDING_STORAGE == \"s3\":\n",
        "        return load_embedding_from_s3(text, model)\n",
        "    elif EMBEDDING_STORAGE == \"local\":\n",
        "        return load_embedding_from_local(text, model)\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown EMBEDDING_STORAGE: {EMBEDDING_STORAGE}\")\n",
        "\n",
        "def save_embedding(text: str, model: str, embedding: List[float]):\n",
        "    \"\"\"Save embedding based on storage setting\"\"\"\n",
        "    if EMBEDDING_STORAGE == \"s3\":\n",
        "        return save_embedding_to_s3(text, model, embedding)\n",
        "    elif EMBEDDING_STORAGE == \"local\":\n",
        "        return save_embedding_to_local(text, model, embedding)\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown EMBEDDING_STORAGE: {EMBEDDING_STORAGE}\")\n",
        "\n",
        "print(\"‚úÖ Embeddings cache ready (with retry logic)\")\n",
        "print(f\"   Storage: {EMBEDDING_STORAGE}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8237e2ac",
      "metadata": {
        "id": "8237e2ac"
      },
      "source": [
        "## Embedding Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "id": "4938efe6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4938efe6",
        "outputId": "0795b0f7-1ca4-4226-ef75-3f25a5352b0a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Embedding generation ready\n"
          ]
        }
      ],
      "source": [
        "def get_embeddings_batch(texts: List[str], model: str = EMBEDDING_MODEL, batch_size: int = 1000) -> List[List[float]]:\n",
        "    \"\"\"Generate embeddings with caching (S3 or local based on EMBEDDING_STORAGE)\"\"\"\n",
        "    cleaned_texts = [clean_question(t) for t in texts]\n",
        "    embeddings = []\n",
        "    cache_hits = 0\n",
        "    api_calls = 0\n",
        "\n",
        "    print(f\"üîÑ Processing {len(cleaned_texts)} texts...\")\n",
        "\n",
        "    for i in tqdm(range(0, len(cleaned_texts), batch_size), desc=\"Batches\"):\n",
        "        batch_texts = cleaned_texts[i:i+batch_size]\n",
        "        batch_embeddings = []\n",
        "        uncached_texts = []\n",
        "        uncached_indices = []\n",
        "\n",
        "        # Check cache (S3 or local based on setting)\n",
        "        for j, text in enumerate(batch_texts):\n",
        "            cached = load_embedding(text, model)\n",
        "            if cached:\n",
        "                batch_embeddings.append(cached)\n",
        "                cache_hits += 1\n",
        "            else:\n",
        "                batch_embeddings.append(None)\n",
        "                uncached_texts.append(text)\n",
        "                uncached_indices.append(j)\n",
        "\n",
        "        # Generate uncached embeddings\n",
        "        if uncached_texts:\n",
        "            try:\n",
        "                response = client.embeddings.create(model=model, input=uncached_texts)\n",
        "                new_embeddings = [d.embedding for d in response.data]\n",
        "                api_calls += len(uncached_texts)\n",
        "\n",
        "                for idx, emb in zip(uncached_indices, new_embeddings):\n",
        "                    batch_embeddings[idx] = emb\n",
        "                    save_embedding(batch_texts[idx], model, emb)\n",
        "            except Exception as e:\n",
        "                error_logger.log_error(\"Embeddings\", f\"Batch failed\", e)\n",
        "                for idx in uncached_indices:\n",
        "                    batch_embeddings[idx] = [0.0] * EMBEDDING_DIMENSIONS\n",
        "\n",
        "        embeddings.extend(batch_embeddings)\n",
        "\n",
        "    print(f\"‚úÖ Complete! Cache: {cache_hits}/{len(embeddings)} ({cache_hits/len(embeddings)*100:.1f}%), API: {api_calls}\")\n",
        "    return embeddings\n",
        "\n",
        "print(\"‚úÖ Embedding generation ready\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8868d0ba",
      "metadata": {
        "id": "8868d0ba"
      },
      "source": [
        "## Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "id": "7f68989f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 718
        },
        "id": "7f68989f",
        "outputId": "a8f99acc-9c5e-414d-91e6-e0632f16001d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìä Loading topics from Google Sheets...\n",
            "‚úÖ Loaded 120 topics from Google Sheets\n",
            "   Unique topics: 59, Unique subtopics: 118\n",
            "\n",
            "üìÇ Upload Langfuse CSV:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-d0268dc5-e881-4af4-a419-0c2e786543e3\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-d0268dc5-e881-4af4-a419-0c2e786543e3\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving combined.csv to combined (5).csv\n",
            "\n",
            "üìä Loading Langfuse data from combined (5).csv...\n",
            "üîç Filtering malformed rows (containing 'kwargs' AND 'args')...\n",
            "‚úÖ Filtered 101 malformed rows (0.8%)\n",
            "   Remaining: 11890 rows\n",
            "üßπ Cleaning 11890 Langfuse rows...\n",
            "‚úÖ Cleaned data: 11890 rows (0 dropped)\n",
            "   Columns: ['id', 'timestamp', 'input', 'is_acm_question', 'output', 'user_feedback', 'country', 'state', 'city', 'ip_address', 'user_language', 'is_suspicious']\n",
            "   Country data: 519 rows with country info\n",
            "\n",
            "üîÑ Syncing master file with S3...\n",
            "üìÇ Checking S3 for master file...\n",
            "‚úÖ Loaded master from S3: 12262 rows\n",
            "   Timestamp range: 2025-07-01T00:00:00+00:00 to 2025-10-10T16:03:53.771000+00:00\n",
            "üîÄ Merging new data (11890 rows) with master (12262 rows)...\n",
            "   Combined: 24152 rows\n",
            "üßπ Removing duplicates (same question + timestamp)...\n",
            "   Removed 11890 duplicates\n",
            "   Deduplication criteria: input + timestamp\n",
            "   Final: 12262 rows\n",
            "‚òÅÔ∏è  Uploading to S3...\n",
            "‚úÖ Uploaded to S3: s3://byupathway-public/langfuse/langfuse_traces_master.parquet\n",
            "‚úÖ Master file synced: 12262 total rows\n",
            "\n",
            "üìä DATA LOADED:\n",
            "   Topics: 120 (59 unique)\n",
            "   Questions: 12262 (from master file)\n",
            "   With metadata: country=882, timestamp=12262\n",
            "   ACM questions: 156\n",
            "   Errors: 0\n",
            "   Dropped rows: 101\n"
          ]
        }
      ],
      "source": [
        "# Load topics from Google Sheets\n",
        "print(\"üìä Loading topics from Google Sheets...\")\n",
        "topics_df = read_topics_from_google_sheets(GOOGLE_SHEETS_URL)\n",
        "topics_df = preprocess_dataframe(topics_df, 'question')\n",
        "\n",
        "# Upload Langfuse CSV\n",
        "if IN_COLAB:\n",
        "    from google.colab import files\n",
        "    print(\"\\nüìÇ Upload Langfuse CSV:\")\n",
        "    uploaded = files.upload()\n",
        "    langfuse_filename = list(uploaded.keys())[0]\n",
        "else:\n",
        "    langfuse_filename = \"notebook/langfuse_traces.csv\"\n",
        "\n",
        "# Load Langfuse data\n",
        "print(f\"\\nüìä Loading Langfuse data from {langfuse_filename}...\")\n",
        "langfuse_df = pd.read_csv(langfuse_filename)\n",
        "\n",
        "# STEP 1: Filter malformed rows (FIRST STEP - remove kwargs/args errors)\n",
        "langfuse_df = filter_malformed_rows(langfuse_df)\n",
        "\n",
        "# STEP 2: Clean Langfuse data\n",
        "langfuse_clean = clean_langfuse_data(langfuse_df)\n",
        "\n",
        "# STEP 3: Sync with master file (S3 storage)\n",
        "master_df = sync_master_file(langfuse_clean)\n",
        "\n",
        "# Create questions dataframe with all metadata columns preserved (including is_acm_question)\n",
        "required_cols = ['id', 'input', 'timestamp', 'country', 'state', 'city', 'output', 'user_feedback', 'ip_address', 'user_language', 'is_suspicious', 'is_acm_question']\n",
        "available_cols = [col for col in required_cols if col in master_df.columns]\n",
        "questions_df = master_df[available_cols].copy()\n",
        "questions_df = questions_df.rename(columns={'input': 'question'})\n",
        "questions_df = preprocess_dataframe(questions_df, 'question')\n",
        "\n",
        "print(f\"\\nüìä DATA LOADED:\")\n",
        "print(f\"   Topics: {len(topics_df)} ({topics_df['topic'].nunique()} unique)\")\n",
        "print(f\"   Questions: {len(questions_df)} (from master file)\")\n",
        "print(f\"   With metadata: country={questions_df['country'].notna().sum()}, timestamp={questions_df['timestamp'].notna().sum()}\")\n",
        "print(f\"   ACM questions: {questions_df['is_acm_question'].sum() if 'is_acm_question' in questions_df.columns else 'N/A'}\")\n",
        "print(f\"   Errors: {error_logger.get_summary()['total_errors']}\")\n",
        "print(f\"   Dropped rows: {error_logger.get_summary()['total_dropped_rows']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b3ef9941",
      "metadata": {
        "id": "b3ef9941"
      },
      "source": [
        "## Prepare Evaluation Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "id": "3c347fef",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3c347fef",
        "outputId": "36deee46-75a5-45b0-adb9-a596ca3af510"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìù Full mode: 12262 questions\n",
            "üí∞ Estimated cost: $0.0124 (embeddings only)\n"
          ]
        }
      ],
      "source": [
        "if EVAL_MODE == \"sample\":\n",
        "    eval_df = questions_df.sample(n=min(SAMPLE_SIZE, len(questions_df)), random_state=RANDOM_SEED).copy()\n",
        "    print(f\"üìù Sample mode: {len(eval_df)} questions\")\n",
        "else:\n",
        "    eval_df = questions_df.copy()\n",
        "    print(f\"üìù Full mode: {len(eval_df)} questions\")\n",
        "\n",
        "# Cost estimate\n",
        "total_tokens = (len(topics_df) + len(eval_df)) * 50\n",
        "embedding_cost = (total_tokens / 1_000_000) * 0.02\n",
        "print(f\"üí∞ Estimated cost: ${embedding_cost:.4f} (embeddings only)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e251a632",
      "metadata": {
        "id": "e251a632"
      },
      "source": [
        "## Similarity Classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "adab14e9",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "adab14e9",
        "outputId": "48c1fe77-cb42-421a-d085-93c584c57488"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üéØ Similarity Classification (threshold: 0.7)\n",
            "üìä Generating topic embeddings...\n",
            "üîÑ Processing 120 texts...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 40.93it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Complete! Cache: 120/120 (100.0%), API: 0\n",
            "üìä Generating question embeddings...\n",
            "üîÑ Processing 12262 texts...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 13/13 [00:02<00:00,  5.22it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Complete! Cache: 12262/12262 (100.0%), API: 0\n",
            "üîç Classifying 12262 questions...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 33%|‚ñà‚ñà‚ñà‚ñé      | 4058/12262 [02:35<04:43, 28.90it/s]"
          ]
        }
      ],
      "source": [
        "def classify_by_similarity(questions_df: pd.DataFrame, topics_df: pd.DataFrame, threshold: float) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
        "    \"\"\"Classify questions by similarity to existing topics - preserves all metadata columns\"\"\"\n",
        "\n",
        "    print(f\"\\nüéØ Similarity Classification (threshold: {threshold})\")\n",
        "\n",
        "    # Generate embeddings\n",
        "    print(f\"üìä Generating topic embeddings...\")\n",
        "    topic_embeddings = get_embeddings_batch(topics_df['question'].tolist())\n",
        "    topics_df = topics_df.copy()\n",
        "    topics_df['embedding'] = topic_embeddings\n",
        "\n",
        "    print(f\"üìä Generating question embeddings...\")\n",
        "    question_embeddings = get_embeddings_batch(questions_df['question'].tolist())\n",
        "\n",
        "    # Classify\n",
        "    similar = []\n",
        "    remaining = []\n",
        "\n",
        "    print(f\"üîç Classifying {len(questions_df)} questions...\")\n",
        "    for idx, (_, row) in enumerate(tqdm(questions_df.iterrows(), total=len(questions_df))):\n",
        "        question = row['question']\n",
        "        q_emb = question_embeddings[idx]\n",
        "\n",
        "        if not q_emb or len(q_emb) != EMBEDDING_DIMENSIONS:\n",
        "            row_data = row.to_dict()\n",
        "            row_data['embedding'] = [0.0]*EMBEDDING_DIMENSIONS\n",
        "            remaining.append(row_data)\n",
        "            continue\n",
        "\n",
        "        best_sim = 0\n",
        "        best_match = None\n",
        "\n",
        "        for _, topic_row in topics_df.iterrows():\n",
        "            t_emb = topic_row['embedding']\n",
        "            if t_emb and len(t_emb) == EMBEDDING_DIMENSIONS:\n",
        "                sim = 1 - cosine(q_emb, t_emb)\n",
        "                if sim > best_sim:\n",
        "                    best_sim = sim\n",
        "                    best_match = topic_row\n",
        "\n",
        "        if best_sim >= threshold and best_match is not None:\n",
        "            row_data = row.to_dict()\n",
        "            row_data['matched_topic'] = best_match['topic']\n",
        "            row_data['matched_subtopic'] = best_match['subtopic']\n",
        "            row_data['similarity_score'] = best_sim\n",
        "            similar.append(row_data)\n",
        "        else:\n",
        "            row_data = row.to_dict()\n",
        "            row_data['embedding'] = q_emb\n",
        "            remaining.append(row_data)\n",
        "\n",
        "    similar_df = pd.DataFrame(similar)\n",
        "    remaining_df = pd.DataFrame(remaining)\n",
        "\n",
        "    print(f\"\\n‚úÖ Classification complete:\")\n",
        "    print(f\"   Similar: {len(similar_df)} ({len(similar_df)/len(questions_df)*100:.1f}%)\")\n",
        "    print(f\"   Remaining: {len(remaining_df)} ({len(remaining_df)/len(questions_df)*100:.1f}%)\")\n",
        "\n",
        "    return similar_df, remaining_df\n",
        "\n",
        "similar_df, remaining_df = classify_by_similarity(eval_df, topics_df, SIMILARITY_THRESHOLD)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d352ac43",
      "metadata": {
        "id": "d352ac43"
      },
      "source": [
        "## Clustering for New Topics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b483e59f",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "b483e59f"
      },
      "outputs": [],
      "source": [
        "clustered_df = None\n",
        "topic_model = None\n",
        "\n",
        "if len(remaining_df) > 0:\n",
        "    print(f\"\\nüéØ Clustering {len(remaining_df)} remaining questions...\")\n",
        "\n",
        "    from umap import UMAP\n",
        "    from hdbscan import HDBSCAN\n",
        "    from bertopic import BERTopic\n",
        "\n",
        "    embeddings = np.array(remaining_df['embedding'].tolist())\n",
        "\n",
        "    # UMAP\n",
        "    print(f\"üîÑ UMAP reduction to {UMAP_N_COMPONENTS} dimensions...\")\n",
        "    umap_model = UMAP(n_components=UMAP_N_COMPONENTS, min_dist=0.0, metric='cosine', random_state=RANDOM_SEED)\n",
        "    reduced = umap_model.fit_transform(embeddings)\n",
        "\n",
        "    # HDBSCAN\n",
        "    print(f\"üîÑ HDBSCAN clustering (min_size={HDBSCAN_MIN_CLUSTER_SIZE})...\")\n",
        "    hdbscan_model = HDBSCAN(min_cluster_size=HDBSCAN_MIN_CLUSTER_SIZE, metric=\"euclidean\", cluster_selection_method=\"eom\")\n",
        "    clusters = hdbscan_model.fit_predict(reduced)\n",
        "\n",
        "    n_clusters = len([c for c in np.unique(clusters) if c != -1])\n",
        "    n_noise = sum(clusters == -1)\n",
        "    print(f\"‚úÖ Found {n_clusters} clusters, {n_noise} noise points\")\n",
        "\n",
        "    if n_clusters > 0:\n",
        "        # BERTopic\n",
        "        topic_model = BERTopic(embedding_model=None, umap_model=umap_model, hdbscan_model=hdbscan_model, verbose=False)\n",
        "        topics, probs = topic_model.fit_transform(remaining_df['question'].tolist(), embeddings)\n",
        "\n",
        "        clustered_df = remaining_df.copy()\n",
        "        clustered_df['cluster_id'] = clusters\n",
        "        clustered_df['topic_id'] = topics\n",
        "        clustered_df = clustered_df[clustered_df['cluster_id'] != -1]\n",
        "\n",
        "        print(f\"‚úÖ Clustered {len(clustered_df)} questions into {n_clusters} topics\")\n",
        "else:\n",
        "    print(\"\\n‚úÖ All questions matched existing topics - no clustering needed\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "14cf39bc",
      "metadata": {
        "id": "14cf39bc"
      },
      "source": [
        "## Generate Topic Names with GPT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f310a6b",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "3f310a6b"
      },
      "outputs": [],
      "source": [
        "topic_names = {}\n",
        "\n",
        "if clustered_df is not None and len(clustered_df) > 0:\n",
        "    print(f\"\\nü§ñ Generating topic names with {GPT_MODEL}...\")\n",
        "\n",
        "    async def generate_topic_name(questions: List[str], keywords: str = \"\") -> str:\n",
        "        \"\"\"Generate a topic name using GPT-5 for a cluster of questions\"\"\"\n",
        "\n",
        "        # Limit to top 10 questions for context (like insights)\n",
        "        sample_questions = questions[:10]\n",
        "        questions_text = \"\\n\".join([f\"- {q}\" for q in sample_questions])\n",
        "\n",
        "        prompt = f\"\"\"\n",
        "Based on the following student questions and keywords, generate a concise, descriptive topic name.\n",
        "\n",
        "QUESTIONS:\n",
        "{questions_text}\n",
        "\n",
        "KEYWORDS: {keywords}\n",
        "\n",
        "Instructions:\n",
        "- Your answer must be ONLY the topic name (2‚Äì8 words), no extra text.\n",
        "- It should clearly describe the shared theme of the questions.\n",
        "- Avoid generic labels like \"General Questions\" or \"Miscellaneous.\"\n",
        "- Do not include \"Topic name:\" or quotation marks.\n",
        "- Use simple, natural English that sounds clear to a student or teacher.\n",
        "\n",
        "Example:\n",
        "Questions:\n",
        "- When does registration open?\n",
        "- What are the fall 2025 enrollment deadlines?\n",
        "Keywords: registration, deadlines\n",
        "\n",
        "Topic name: Fall 2025 Registration Deadlines\n",
        "\n",
        "Now generate the topic name for the questions above:\n",
        "\"\"\"\n",
        "\n",
        "        try:\n",
        "            messages = [\n",
        "                {\"role\": \"system\", \"content\": \"You are an expert at creating clear, descriptive topic names for student question categories.\"},\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ]\n",
        "\n",
        "            # GPT-5 specific configuration (NO temperature parameter!)\n",
        "            response = await async_client.chat.completions.create(\n",
        "                model=GPT_MODEL,\n",
        "                messages=messages,\n",
        "                max_completion_tokens=1000  # Use max_completion_tokens for GPT-5, not max_tokens\n",
        "            )\n",
        "\n",
        "            topic_name = response.choices[0].message.content.strip()\n",
        "\n",
        "            # Clean up the response\n",
        "            topic_name = topic_name.replace(\"Topic name:\", \"\").strip()\n",
        "            topic_name = topic_name.strip('\\\"\\'')\n",
        "\n",
        "            if not topic_name:\n",
        "                topic_name = f\"Topic: {keywords[:50]}\" if keywords else f\"Question Group {hash(str(questions[:3])) % 1000}\"\n",
        "\n",
        "            return topic_name\n",
        "\n",
        "        except Exception as e:\n",
        "            error_logger.log_error(\"TopicNaming\", f\"GPT failed: {str(e)}\", e)\n",
        "            # Fallback to keyword-based name\n",
        "            fallback_name = f\"Topic: {keywords[:50]}\" if keywords else f\"Question Group {hash(str(questions[:3])) % 1000}\"\n",
        "            return fallback_name\n",
        "\n",
        "    async def process_all_clusters():\n",
        "        tasks = []\n",
        "        cluster_ids = []\n",
        "\n",
        "        for cluster_id, group in clustered_df.groupby('cluster_id'):\n",
        "            questions = group['question'].tolist()\n",
        "            # Extract keywords from BERTopic if available\n",
        "            keywords = group['topic_keywords'].iloc[0] if 'topic_keywords' in group.columns else \"\"\n",
        "\n",
        "            tasks.append(generate_topic_name(questions, keywords))\n",
        "            cluster_ids.append(cluster_id)\n",
        "\n",
        "        names = await asyncio.gather(*tasks)\n",
        "        return dict(zip(cluster_ids, names))\n",
        "\n",
        "    topic_names = await process_all_clusters()\n",
        "    clustered_df['topic_name'] = clustered_df['cluster_id'].map(topic_names)\n",
        "\n",
        "    print(f\"‚úÖ Generated {len(topic_names)} topic names\")\n",
        "    for cid, name in list(topic_names.items())[:5]:\n",
        "        count = len(clustered_df[clustered_df['cluster_id'] == cid])\n",
        "        print(f\"   {name} ({count} questions)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f30620d7",
      "metadata": {
        "id": "f30620d7"
      },
      "source": [
        "## Generate Output Files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cdc0920e",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "cdc0920e"
      },
      "outputs": [],
      "source": [
        "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "\n",
        "print(f\"\\nüìÅ Generating output files...\")\n",
        "\n",
        "# File 1: Similar questions\n",
        "file1 = f\"similar_questions_{timestamp}.parquet\"\n",
        "if len(similar_df) > 0:\n",
        "    output1 = similar_df[['question', 'matched_topic', 'matched_subtopic', 'similarity_score']].copy()\n",
        "    output1.columns = ['question', 'existing_topic', 'existing_subtopic', 'similarity_score']\n",
        "    output1 = output1.sort_values('similarity_score', ascending=False)\n",
        "else:\n",
        "    output1 = pd.DataFrame(columns=['question', 'existing_topic', 'existing_subtopic', 'similarity_score'])\n",
        "\n",
        "# Add metadata\n",
        "output1.attrs['metadata'] = {\n",
        "    'timestamp': timestamp,\n",
        "    'threshold': SIMILARITY_THRESHOLD,\n",
        "    'total_questions': len(output1),\n",
        "    'default_visible_columns': ['question', 'existing_topic', 'existing_subtopic', 'similarity_score']\n",
        "}\n",
        "output1.to_parquet(file1)\n",
        "print(f\"‚úÖ {file1}: {len(output1)} rows\")\n",
        "\n",
        "# File 2: New topics\n",
        "file2 = f\"new_topics_{timestamp}.parquet\"\n",
        "if clustered_df is not None and len(clustered_df) > 0:\n",
        "    cluster_summary = clustered_df.groupby('cluster_id').agg({\n",
        "        'topic_name': 'first',\n",
        "        'question': ['first', 'count']\n",
        "    }).reset_index()\n",
        "    cluster_summary.columns = ['cluster_id', 'topic_name', 'representative_question', 'question_count']\n",
        "    output2 = cluster_summary[['topic_name', 'representative_question', 'question_count']].sort_values('question_count', ascending=False)\n",
        "else:\n",
        "    output2 = pd.DataFrame(columns=['topic_name', 'representative_question', 'question_count'])\n",
        "\n",
        "output2.attrs['metadata'] = {\n",
        "    'timestamp': timestamp,\n",
        "    'total_topics': len(output2),\n",
        "    'default_visible_columns': ['topic_name', 'representative_question', 'question_count']\n",
        "}\n",
        "output2.to_parquet(file2)\n",
        "print(f\"‚úÖ {file2}: {len(output2)} rows\")\n",
        "\n",
        "# File 3: All questions review WITH METADATA\n",
        "file3 = f\"pathway_questions_review_{timestamp}.parquet\"\n",
        "review_data = []\n",
        "\n",
        "# Metadata columns to preserve (including is_acm_question for Streamlit filtering)\n",
        "metadata_cols = ['timestamp', 'country', 'state', 'city', 'output', 'user_feedback', 'ip_address', 'user_language', 'is_suspicious', 'is_acm_question']\n",
        "\n",
        "if len(similar_df) > 0:\n",
        "    for _, row in similar_df.iterrows():\n",
        "        record = {\n",
        "            'question': row['question'],\n",
        "            'topic_name': f\"{row['matched_topic']} | {row['matched_subtopic']}\",\n",
        "            'classification': 'existing',\n",
        "            'confidence': row['similarity_score']\n",
        "        }\n",
        "        # Add metadata columns\n",
        "        for col in metadata_cols:\n",
        "            record[col] = row.get(col, None)\n",
        "        review_data.append(record)\n",
        "\n",
        "if clustered_df is not None and len(clustered_df) > 0:\n",
        "    for _, row in clustered_df.iterrows():\n",
        "        record = {\n",
        "            'question': row['question'],\n",
        "            'topic_name': row['topic_name'],\n",
        "            'classification': 'new',\n",
        "            'confidence': 0.5\n",
        "        }\n",
        "        # Add metadata columns\n",
        "        for col in metadata_cols:\n",
        "            record[col] = row.get(col, None)\n",
        "        review_data.append(record)\n",
        "\n",
        "if len(remaining_df) > len(clustered_df) if clustered_df is not None else len(remaining_df) > 0:\n",
        "    clustered_questions = set(clustered_df['question']) if clustered_df is not None else set()\n",
        "    for _, row in remaining_df.iterrows():\n",
        "        if row['question'] not in clustered_questions:\n",
        "            record = {\n",
        "                'question': row['question'],\n",
        "                'topic_name': 'Other',\n",
        "                'classification': 'uncategorized',\n",
        "                'confidence': 0.0\n",
        "            }\n",
        "            # Add metadata columns\n",
        "            for col in metadata_cols:\n",
        "                record[col] = row.get(col, None)\n",
        "            review_data.append(record)\n",
        "\n",
        "output3 = pd.DataFrame(review_data)\n",
        "output3.attrs['metadata'] = {\n",
        "    'timestamp': timestamp,\n",
        "    'total_questions': len(output3),\n",
        "    'default_visible_columns': ['question', 'timestamp', 'country', 'state', 'topic_name', 'classification', 'is_acm_question']\n",
        "}\n",
        "output3.to_parquet(file3)\n",
        "print(f\"‚úÖ {file3}: {len(output3)} rows\")\n",
        "print(f\"   Columns: {list(output3.columns)}\")\n",
        "print(f\"   With country data: {output3['country'].notna().sum()} rows\")\n",
        "print(f\"   ACM questions: {output3['is_acm_question'].sum() if 'is_acm_question' in output3.columns else 'N/A'}\")\n",
        "\n",
        "# File 4: Topic distribution analytics\n",
        "file4 = f\"topic_distribution_{timestamp}.parquet\"\n",
        "topic_dist = output3.groupby(['topic_name', 'classification']).size().reset_index(name='count')\n",
        "topic_dist = topic_dist.sort_values('count', ascending=False)\n",
        "topic_dist.attrs['metadata'] = {\n",
        "    'timestamp': timestamp,\n",
        "    'total_topics': len(topic_dist),\n",
        "    'default_visible_columns': ['topic_name', 'classification', 'count']\n",
        "}\n",
        "topic_dist.to_parquet(file4)\n",
        "print(f\"‚úÖ {file4}: {len(topic_dist)} rows\")\n",
        "\n",
        "# Error log\n",
        "error_log_file = f\"error_log_{timestamp}.json\"\n",
        "with open(error_log_file, 'w') as f:\n",
        "    json.dump(error_logger.get_summary(), f, indent=2)\n",
        "print(f\"‚úÖ {error_log_file}: error summary\")\n",
        "\n",
        "# Create output_files list for S3 upload\n",
        "output_files = [file1, file2, file3, file4, error_log_file]\n",
        "\n",
        "print(f\"\\nüì¶ OUTPUT FILES GENERATED:\")\n",
        "print(f\"   1. {file1} - Questions matched to existing topics\")\n",
        "print(f\"   2. {file2} - New topic clusters discovered\")\n",
        "print(f\"   3. {file3} - Complete review with ALL METADATA\")\n",
        "print(f\"   4. {file4} - Topic distribution analytics\")\n",
        "print(f\"   5. {error_log_file} - Error log\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9a7625c6",
      "metadata": {
        "id": "9a7625c6"
      },
      "source": [
        "## Upload to S3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a9e9af3a",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "a9e9af3a"
      },
      "outputs": [],
      "source": [
        "# Skip ACL if you don't have PutObjectAcl permission\n",
        "# Use this if you can upload without ACL but not with public-read\n",
        "\n",
        "print(f\"\\n‚òÅÔ∏è  Uploading to S3 (without public ACL)...\")\n",
        "\n",
        "# Delete old files\n",
        "try:\n",
        "    delete_s3_folder(S3_OUTPUT_PREFIX)\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è  Could not delete old files: {e}\")\n",
        "\n",
        "# Upload new files WITHOUT public-read ACL\n",
        "uploaded = []\n",
        "failed = []\n",
        "\n",
        "for filepath in output_files:\n",
        "    if not os.path.exists(filepath):\n",
        "        print(f\"‚ùå File not found: {filepath}\")\n",
        "        failed.append(filepath)\n",
        "        continue\n",
        "\n",
        "    file_size = os.path.getsize(filepath)\n",
        "    print(f\"üì§ Uploading {filepath} ({file_size:,} bytes)...\")\n",
        "\n",
        "    s3_key = f\"{S3_OUTPUT_PREFIX}/{filepath}\"\n",
        "\n",
        "    try:\n",
        "        # Use public=False to skip ACL (if you don't have PutObjectAcl permission)\n",
        "        if upload_to_s3(filepath, s3_key, public=False):\n",
        "            # Note: URL won't be publicly accessible without ACL\n",
        "            url = f\"s3://{S3_BUCKET}/{s3_key}\"\n",
        "            uploaded.append(url)\n",
        "            print(f\"   ‚úÖ Success: {url}\")\n",
        "        else:\n",
        "            failed.append(filepath)\n",
        "            print(f\"   ‚ùå Failed: {filepath}\")\n",
        "    except Exception as e:\n",
        "        print(f\"   ‚ùå Exception: {str(e)}\")\n",
        "        failed.append(filepath)\n",
        "\n",
        "print(f\"\\nüìä UPLOAD SUMMARY:\")\n",
        "print(f\"   ‚úÖ Successful: {len(uploaded)}/{len(output_files)}\")\n",
        "print(f\"   ‚ùå Failed: {len(failed)}/{len(output_files)}\")\n",
        "\n",
        "if uploaded:\n",
        "    print(f\"\\n‚úÖ Uploaded files (private - not publicly accessible):\")\n",
        "    for url in uploaded:\n",
        "        print(f\"   {url}\")\n",
        "    print(f\"\\nüí° TIP: Files are uploaded but not public. Your Streamlit app can access them with AWS credentials.\")\n",
        "\n",
        "if failed:\n",
        "    print(f\"\\n‚ùå Failed files:\")\n",
        "    for f in failed:\n",
        "        print(f\"   {f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b7865c4b",
      "metadata": {
        "id": "b7865c4b"
      },
      "source": [
        "## Analysis & Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "36d64886",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "36d64886"
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "fig.suptitle('Hybrid Topic Discovery Analysis', fontsize=16, fontweight='bold')\n",
        "\n",
        "# 1. Processing pipeline\n",
        "pipeline = ['Total', 'Similar', 'New Topics', 'Uncategorized']\n",
        "counts = [\n",
        "    len(eval_df),\n",
        "    len(similar_df),\n",
        "    len(clustered_df) if clustered_df is not None else 0,\n",
        "    len(eval_df) - len(similar_df) - (len(clustered_df) if clustered_df is not None else 0)\n",
        "]\n",
        "axes[0,0].bar(pipeline, counts, color=['lightblue', 'lightgreen', 'orange', 'lightcoral'])\n",
        "axes[0,0].set_title('Processing Results')\n",
        "axes[0,0].set_ylabel('Questions')\n",
        "for i, (label, count) in enumerate(zip(pipeline, counts)):\n",
        "    axes[0,0].text(i, count + max(counts)*0.01, f\"{count}\\n({count/len(eval_df)*100:.1f}%)\", ha='center')\n",
        "\n",
        "# 2. Similarity distribution\n",
        "if len(similar_df) > 0:\n",
        "    axes[0,1].hist(similar_df['similarity_score'], bins=20, alpha=0.7, color='lightgreen', edgecolor='black')\n",
        "    axes[0,1].axvline(SIMILARITY_THRESHOLD, color='red', linestyle='--', label=f'Threshold: {SIMILARITY_THRESHOLD}')\n",
        "    axes[0,1].set_xlabel('Similarity Score')\n",
        "    axes[0,1].set_ylabel('Count')\n",
        "    axes[0,1].set_title('Similarity Distribution')\n",
        "    axes[0,1].legend()\n",
        "else:\n",
        "    axes[0,1].text(0.5, 0.5, 'No similar questions', ha='center', va='center', transform=axes[0,1].transAxes)\n",
        "    axes[0,1].set_title('Similarity Distribution')\n",
        "\n",
        "# 3. Cluster sizes\n",
        "if clustered_df is not None and len(clustered_df) > 0:\n",
        "    cluster_sizes = clustered_df['cluster_id'].value_counts().values\n",
        "    axes[1,0].hist(cluster_sizes, bins=min(20, len(cluster_sizes)), alpha=0.7, color='orange', edgecolor='black')\n",
        "    axes[1,0].set_xlabel('Cluster Size')\n",
        "    axes[1,0].set_ylabel('Count')\n",
        "    axes[1,0].set_title('New Topic Sizes')\n",
        "else:\n",
        "    axes[1,0].text(0.5, 0.5, 'No clusters', ha='center', va='center', transform=axes[1,0].transAxes)\n",
        "    axes[1,0].set_title('New Topic Sizes')\n",
        "\n",
        "# 4. Topic distribution pie\n",
        "pie_data = output3['classification'].value_counts()\n",
        "if len(pie_data) > 0:\n",
        "    axes[1,1].pie(pie_data.values, labels=pie_data.index, autopct='%1.1f%%', startangle=90)\n",
        "    axes[1,1].set_title('Classification Distribution')\n",
        "else:\n",
        "    axes[1,1].text(0.5, 0.5, 'No data', ha='center', va='center', transform=axes[1,1].transAxes)\n",
        "    axes[1,1].set_title('Classification Distribution')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nüìä SUMMARY:\")\n",
        "print(f\"   Total processed: {len(eval_df)}\")\n",
        "print(f\"   Similar to existing: {len(similar_df)} ({len(similar_df)/len(eval_df)*100:.1f}%)\")\n",
        "print(f\"   New topics: {len(topic_names)}\")\n",
        "print(f\"   Errors: {error_logger.get_summary()['total_errors']}\")\n",
        "print(f\"   Warnings: {error_logger.get_summary()['total_warnings']}\")\n",
        "print(f\"\\n‚úÖ COMPLETE! Files uploaded to S3.\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}