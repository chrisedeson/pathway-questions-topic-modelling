{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4eb2ca6b",
   "metadata": {},
   "source": [
    "# Hybrid Topic Discovery & Classification with AWS Integration\n",
    "\n",
    "**Purpose**: Classify student questions against existing topics and discover new topics using hybrid approach.\n",
    "\n",
    "**Data Flow**:\n",
    "1. Load topics from Google Sheets\n",
    "2. Load student questions from Langfuse CSV\n",
    "3. Similarity classification (threshold-based)\n",
    "4. Clustering for new topic discovery\n",
    "5. Output parquet files to AWS S3\n",
    "\n",
    "**Key Features**:\n",
    "- AWS S3 for embeddings cache and outputs\n",
    "- Environment-responsive configuration\n",
    "- Comprehensive error logging\n",
    "- Analytics outputs for Streamlit dashboard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e89b193",
   "metadata": {},
   "source": [
    "## Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8a424b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q openai pandas numpy scipy scikit-learn matplotlib seaborn tqdm umap-learn hdbscan bertopic backoff boto3 gspread oauth2client pyarrow fastparquet python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dca8fb7",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa6d765",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing settings\n",
    "EVAL_MODE = \"sample\"  # \"sample\" or \"all\"\n",
    "SAMPLE_SIZE = 1000\n",
    "SIMILARITY_THRESHOLD = 0.70\n",
    "REPRESENTATIVE_QUESTION_METHOD = \"centroid\"  # \"centroid\" or \"frequent\"\n",
    "\n",
    "# Model settings\n",
    "EMBEDDING_MODEL = \"text-embedding-3-small\"\n",
    "EMBEDDING_DIMENSIONS = 1536\n",
    "GPT_MODEL = \"gpt-5-nano\"\n",
    "\n",
    "# AWS S3 settings\n",
    "S3_BUCKET = \"byupathway-public\"\n",
    "S3_OUTPUT_PREFIX = \"topic-modeling-data\"\n",
    "S3_CACHE_PREFIX = \"embeddings-cache\"\n",
    "S3_REGION = \"us-east-1\"\n",
    "\n",
    "# Clustering settings\n",
    "UMAP_N_COMPONENTS = 5\n",
    "HDBSCAN_MIN_CLUSTER_SIZE = 3\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# Google Sheets URL (hardcoded with fallback)\n",
    "GOOGLE_SHEETS_URL = \"https://docs.google.com/spreadsheets/d/1L3kOmE6mZEVotjUu2ZuLqir2rZ4c0yfNeeTuHwf3JQI/edit?gid=0#gid=0\"\n",
    "\n",
    "print(\"âœ… Configuration loaded\")\n",
    "print(f\"   Mode: {EVAL_MODE}, Threshold: {SIMILARITY_THRESHOLD}\")\n",
    "print(f\"   S3 Bucket: {S3_BUCKET}\")\n",
    "print(f\"   Embedding Model: {EMBEDDING_MODEL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2988c527",
   "metadata": {},
   "source": [
    "## Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89070b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import cosine\n",
    "import json\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import time\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import List, Dict, Tuple, Optional, Any\n",
    "import asyncio\n",
    "import backoff\n",
    "import re\n",
    "import hashlib\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "import gspread\n",
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "\n",
    "# Detect environment\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "    from google.colab import userdata\n",
    "    print(\"ðŸ”§ Running in Google Colab\")\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "    from dotenv import load_dotenv\n",
    "    load_dotenv()\n",
    "    print(\"ðŸ”§ Running locally\")\n",
    "\n",
    "# Load credentials\n",
    "if IN_COLAB:\n",
    "    OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n",
    "    AWS_ACCESS_KEY = userdata.get('AWS_ACCESS_KEY_ID')\n",
    "    AWS_SECRET_KEY = userdata.get('AWS_SECRET_ACCESS_KEY')\n",
    "    GOOGLE_SERVICE_ACCOUNT = userdata.get('GOOGLE_SERVICE_ACCOUNT_JSON')\n",
    "else:\n",
    "    OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "    AWS_ACCESS_KEY = os.getenv('AWS_ACCESS_KEY_ID')\n",
    "    AWS_SECRET_KEY = os.getenv('AWS_SECRET_ACCESS_KEY')\n",
    "    GOOGLE_SERVICE_ACCOUNT = os.getenv('GOOGLE_SERVICE_ACCOUNT_JSON')\n",
    "\n",
    "# Initialize OpenAI\n",
    "from openai import OpenAI, AsyncOpenAI\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "async_client = AsyncOpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "# Initialize AWS S3\n",
    "s3_client = boto3.client(\n",
    "    's3',\n",
    "    aws_access_key_id=AWS_ACCESS_KEY,\n",
    "    aws_secret_access_key=AWS_SECRET_KEY,\n",
    "    region_name=S3_REGION\n",
    ")\n",
    "\n",
    "print(\"âœ… Environment setup complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6199ff1c",
   "metadata": {},
   "source": [
    "## Error Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f9701f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ErrorLogger:\n",
    "    def __init__(self):\n",
    "        self.errors = []\n",
    "        self.warnings = []\n",
    "        self.rows_dropped = []\n",
    "        self.timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        \n",
    "    def log_error(self, stage: str, message: str, data: Any = None):\n",
    "        entry = {\"timestamp\": datetime.now().isoformat(), \"stage\": stage, \"message\": message, \"data\": str(data)}\n",
    "        self.errors.append(entry)\n",
    "        print(f\"âŒ ERROR [{stage}]: {message}\")\n",
    "        \n",
    "    def log_warning(self, stage: str, message: str, data: Any = None):\n",
    "        entry = {\"timestamp\": datetime.now().isoformat(), \"stage\": stage, \"message\": message, \"data\": str(data)}\n",
    "        self.warnings.append(entry)\n",
    "        print(f\"âš ï¸  WARNING [{stage}]: {message}\")\n",
    "        \n",
    "    def log_dropped_row(self, stage: str, reason: str, row_data: Any):\n",
    "        entry = {\"timestamp\": datetime.now().isoformat(), \"stage\": stage, \"reason\": reason, \"row_data\": str(row_data)}\n",
    "        self.rows_dropped.append(entry)\n",
    "        \n",
    "    def get_summary(self):\n",
    "        return {\n",
    "            \"total_errors\": len(self.errors),\n",
    "            \"total_warnings\": len(self.warnings),\n",
    "            \"total_dropped_rows\": len(self.rows_dropped),\n",
    "            \"errors\": self.errors,\n",
    "            \"warnings\": self.warnings,\n",
    "            \"dropped_rows\": self.rows_dropped\n",
    "        }\n",
    "    \n",
    "    def save_to_file(self, filename: str):\n",
    "        with open(filename, 'w') as f:\n",
    "            json.dump(self.get_summary(), f, indent=2)\n",
    "        print(f\"ðŸ“ Error log saved: {filename}\")\n",
    "        return filename\n",
    "\n",
    "error_logger = ErrorLogger()\n",
    "print(\"âœ… Error logger initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40fc8e3",
   "metadata": {},
   "source": [
    "## AWS S3 Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3d272b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@backoff.on_exception(\n",
    "    backoff.expo,\n",
    "    Exception,\n",
    "    max_tries=5,\n",
    "    max_time=30,\n",
    "    giveup=lambda e: isinstance(e, (KeyboardInterrupt, SystemExit))\n",
    ")\n",
    "def upload_to_s3(local_path: str, s3_key: str, public: bool = True) -> bool:\n",
    "    \"\"\"Upload file to S3 with retry logic and exponential backoff\n",
    "    \n",
    "    Args:\n",
    "        local_path: Local file path to upload\n",
    "        s3_key: S3 key (path in bucket)\n",
    "        public: Whether to set public-read ACL (default True)\n",
    "    \n",
    "    Returns:\n",
    "        bool: True if successful, False otherwise\n",
    "    \"\"\"\n",
    "    try:\n",
    "        extra_args = {'ACL': 'public-read'} if public else {}\n",
    "        s3_client.upload_file(local_path, S3_BUCKET, s3_key, ExtraArgs=extra_args)\n",
    "        \n",
    "        if public:\n",
    "            url = f\"https://{S3_BUCKET}.s3.amazonaws.com/{s3_key}\"\n",
    "            print(f\"âœ… Uploaded to S3: {url}\")\n",
    "        else:\n",
    "            print(f\"âœ… Uploaded to S3: s3://{S3_BUCKET}/{s3_key}\")\n",
    "        \n",
    "        return True\n",
    "    except Exception as e:\n",
    "        error_logger.log_error(\"S3_Upload\", f\"Failed to upload {local_path} after retries\", e)\n",
    "        return False\n",
    "\n",
    "@backoff.on_exception(\n",
    "    backoff.expo,\n",
    "    Exception,\n",
    "    max_tries=3,\n",
    "    max_time=15,\n",
    "    giveup=lambda e: isinstance(e, (KeyboardInterrupt, SystemExit))\n",
    ")\n",
    "def download_from_s3(s3_key: str, local_path: str) -> bool:\n",
    "    \"\"\"Download file from S3 with retry logic\"\"\"\n",
    "    try:\n",
    "        s3_client.download_file(S3_BUCKET, s3_key, local_path)\n",
    "        return True\n",
    "    except ClientError as e:\n",
    "        if e.response['Error']['Code'] == '404':\n",
    "            # File doesn't exist - don't retry\n",
    "            return False\n",
    "        error_logger.log_error(\"S3_Download\", f\"Failed to download {s3_key} after retries\", e)\n",
    "        return False\n",
    "\n",
    "def delete_s3_folder(prefix: str):\n",
    "    \"\"\"Delete all objects with given prefix\"\"\"\n",
    "    try:\n",
    "        response = s3_client.list_objects_v2(Bucket=S3_BUCKET, Prefix=prefix)\n",
    "        if 'Contents' in response:\n",
    "            objects = [{'Key': obj['Key']} for obj in response['Contents']]\n",
    "            s3_client.delete_objects(Bucket=S3_BUCKET, Delete={'Objects': objects})\n",
    "            print(f\"ðŸ—‘ï¸  Deleted {len(objects)} objects from s3://{S3_BUCKET}/{prefix}\")\n",
    "    except Exception as e:\n",
    "        error_logger.log_error(\"S3_Delete\", f\"Failed to delete folder {prefix}\", e)\n",
    "\n",
    "print(\"âœ… AWS S3 utilities ready (with retry logic)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9720ab4",
   "metadata": {},
   "source": [
    "### âœ¨ Retry Logic Features\n",
    "\n",
    "**Automatic Retry with Exponential Backoff:**\n",
    "- **`upload_to_s3`**: Up to 5 retries over 30 seconds for uploads\n",
    "- **`download_from_s3`**: Up to 3 retries over 15 seconds for downloads\n",
    "- **Exponential backoff**: Waits longer between each retry attempt\n",
    "- **Graceful degradation**: Cache failures don't break the main workflow\n",
    "\n",
    "**What this fixes:**\n",
    "- âœ… Handles temporary network glitches\n",
    "- âœ… Manages AWS rate limiting\n",
    "- âœ… Resolves race conditions from concurrent uploads\n",
    "- âœ… Automatic cleanup of temp files\n",
    "\n",
    "**Performance:**\n",
    "- Cache uploads are non-blocking - failures are logged but don't stop processing\n",
    "- Downloads check for 404 (file not found) and don't retry unnecessarily\n",
    "- Main output file uploads get full retry protection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f833c4",
   "metadata": {},
   "source": [
    "## Google Sheets Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ded2f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_topics_from_google_sheets(sheet_url: str) -> pd.DataFrame:\n",
    "    \"\"\"Read topics from Google Sheets with flexible column handling\"\"\"\n",
    "    try:\n",
    "        creds_dict = json.loads(GOOGLE_SERVICE_ACCOUNT)\n",
    "        scope = ['https://spreadsheets.google.com/feeds', 'https://www.googleapis.com/auth/drive']\n",
    "        creds = ServiceAccountCredentials.from_json_keyfile_dict(creds_dict, scope)\n",
    "        gc = gspread.authorize(creds)\n",
    "        \n",
    "        sheet = gc.open_by_url(sheet_url)\n",
    "        worksheet = sheet.get_worksheet(0)\n",
    "        data = worksheet.get_all_records()\n",
    "        df = pd.DataFrame(data)\n",
    "        \n",
    "        # Handle both uppercase and lowercase column names\n",
    "        column_mapping = {}\n",
    "        for col in df.columns:\n",
    "            col_lower = col.lower()\n",
    "            if col_lower == 'topics' or col_lower == 'topic':\n",
    "                column_mapping[col] = 'topic'\n",
    "            elif col_lower == 'subtopics' or col_lower == 'subtopic':\n",
    "                column_mapping[col] = 'subtopic'\n",
    "            elif col_lower == 'questions' or col_lower == 'question':\n",
    "                column_mapping[col] = 'question'\n",
    "        \n",
    "        df = df.rename(columns=column_mapping)\n",
    "        \n",
    "        required = ['topic', 'subtopic', 'question']\n",
    "        if not all(col in df.columns for col in required):\n",
    "            raise ValueError(f\"Missing required columns. Found: {list(df.columns)}\")\n",
    "        \n",
    "        df = df[required].dropna()\n",
    "        print(f\"âœ… Loaded {len(df)} topics from Google Sheets\")\n",
    "        print(f\"   Unique topics: {df['topic'].nunique()}, Unique subtopics: {df['subtopic'].nunique()}\")\n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_logger.log_error(\"GoogleSheets\", \"Failed to read topics\", e)\n",
    "        raise\n",
    "\n",
    "print(\"âœ… Google Sheets integration ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c2f81d",
   "metadata": {},
   "source": [
    "## Langfuse Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c220e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data_from_kwargs(kwargs_str: str) -> tuple[str, str]:\n",
    "    \"\"\"Extract question and response from malformed kwargs JSON string\"\"\"\n",
    "    try:\n",
    "        # Find positions\n",
    "        request_pos = kwargs_str.find('\"request\":')\n",
    "        if request_pos == -1:\n",
    "            return \"\", \"\"\n",
    "        \n",
    "        # Find all \"data\": positions before request\n",
    "        data_positions = []\n",
    "        start = 0\n",
    "        while True:\n",
    "            pos = kwargs_str.find('\"data\":', start)\n",
    "            if pos == -1 or pos >= request_pos:\n",
    "                break\n",
    "            data_positions.append(pos)\n",
    "            start = pos + 1\n",
    "        \n",
    "        if not data_positions:\n",
    "            return \"\", \"\"\n",
    "        \n",
    "        # Extract question from the last \"data\" (which has \"question\" field)\n",
    "        last_data_pos = data_positions[-1]\n",
    "        data_section_end = request_pos\n",
    "        data_section = kwargs_str[last_data_pos:data_section_end].strip()\n",
    "        if data_section.endswith(','):\n",
    "            data_section = data_section[:-1]\n",
    "        \n",
    "        # Try to extract question from \"question\" field\n",
    "        question_match = re.search(r'\"question\"\\s*:\\s*\"([^\"]*)\"', data_section)\n",
    "        question = question_match.group(1) if question_match else \"\"\n",
    "        \n",
    "        # Extract response from the first \"data\" (which has messages)\n",
    "        first_data_pos = data_positions[0]\n",
    "        next_data_pos = data_positions[1] if len(data_positions) > 1 else request_pos\n",
    "        first_data_section = kwargs_str[first_data_pos:next_data_pos].strip()\n",
    "        if first_data_section.endswith(','):\n",
    "            first_data_section = first_data_section[:-1]\n",
    "        \n",
    "        # Extract last assistant message from messages\n",
    "        response = \"\"\n",
    "        assistant_positions = []\n",
    "        start = 0\n",
    "        while True:\n",
    "            pos = first_data_section.find('\"role\": \"assistant\"', start)\n",
    "            if pos == -1:\n",
    "                break\n",
    "            assistant_positions.append(pos)\n",
    "            start = pos + 1\n",
    "        \n",
    "        if assistant_positions:\n",
    "            last_assistant_pos = assistant_positions[-1]\n",
    "            content_start = first_data_section.find('\"content\": \"', last_assistant_pos)\n",
    "            if content_start != -1:\n",
    "                content_start += len('\"content\": \"')\n",
    "                content_end = content_start\n",
    "                while content_end < len(first_data_section):\n",
    "                    if first_data_section[content_end] == '\"' and (content_end == 0 or first_data_section[content_end-1] != '\\\\'):\n",
    "                        break\n",
    "                    content_end += 1\n",
    "                response = first_data_section[content_start:content_end]\n",
    "        \n",
    "        return question, response\n",
    "    \n",
    "    except Exception as e:\n",
    "        return \"\", \"\"\n",
    "\n",
    "def clean_langfuse_data(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Clean Langfuse CSV with kwargs error handling\"\"\"\n",
    "    print(f\"ðŸ§¹ Cleaning {len(df)} rows from Langfuse...\")\n",
    "    \n",
    "    cleaned_rows = []\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        try:\n",
    "            # Extract question from kwargs if needed\n",
    "            if pd.isna(row.get('input')) or row.get('input') == '' or '\"kwargs\"' in str(row.get('input', '')):\n",
    "                if '\"kwargs\"' in str(row.get('input', '')):\n",
    "                    # This is a kwargs row - extract data from the malformed JSON\n",
    "                    question, response = extract_data_from_kwargs(str(row.get('input', '')))\n",
    "                    if question:\n",
    "                        row = row.copy()\n",
    "                        row['input'] = question\n",
    "                        if response and (pd.isna(row.get('output')) or row.get('output') == ''):\n",
    "                            row['output'] = response\n",
    "                elif 'kwargs' in df.columns and pd.notna(row.get('kwargs')):\n",
    "                    try:\n",
    "                        kwargs_data = json.loads(row['kwargs'])\n",
    "                        messages = kwargs_data.get('data', {}).get('messages', [])\n",
    "                        if messages and len(messages) > 0:\n",
    "                            content = messages[0].get('content', '')\n",
    "                            row = row.copy()\n",
    "                            row['input'] = content\n",
    "                    except (json.JSONDecodeError, KeyError, IndexError) as e:\n",
    "                        error_logger.log_dropped_row(\"LangfuseClean\", \"Invalid kwargs JSON\", {\"index\": idx, \"kwargs\": str(row.get('kwargs', ''))[:100]})\n",
    "                        continue\n",
    "            \n",
    "            # Validate required fields\n",
    "            if pd.isna(row.get('input')) or row.get('input') == '':\n",
    "                error_logger.log_dropped_row(\"LangfuseClean\", \"Missing input\", {\"index\": idx})\n",
    "                continue\n",
    "            \n",
    "            cleaned_rows.append(row)\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_logger.log_dropped_row(\"LangfuseClean\", f\"Unexpected error: {e}\", {\"index\": idx})\n",
    "            continue\n",
    "    \n",
    "    cleaned_df = pd.DataFrame(cleaned_rows)\n",
    "    \n",
    "    # Remove duplicates (same timestamp and question)\n",
    "    if 'timestamp' in cleaned_df.columns and 'input' in cleaned_df.columns:\n",
    "        before_dedup = len(cleaned_df)\n",
    "        cleaned_df = cleaned_df.drop_duplicates(subset=['timestamp', 'input'], keep='first')\n",
    "        after_dedup = len(cleaned_df)\n",
    "        if before_dedup > after_dedup:\n",
    "            print(f\"ðŸ—‘ï¸  Removed {before_dedup - after_dedup} duplicate rows\")\n",
    "    \n",
    "    print(f\"âœ… Cleaned data: {len(cleaned_df)} rows ({len(df) - len(cleaned_df)} dropped)\")\n",
    "    return cleaned_df\n",
    "\n",
    "print(\"âœ… Langfuse cleaning utilities ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5469ad7d",
   "metadata": {},
   "source": [
    "## Question Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c50dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_question(question: str) -> str:\n",
    "    \"\"\"Remove ACM prefixes and clean text\"\"\"\n",
    "    if not isinstance(question, str):\n",
    "        return str(question) if question is not None else \"\"\n",
    "    \n",
    "    pattern = r'^\\s*\\(ACMs?\\s+[Qq]uestion\\)\\s*:?\\s*'\n",
    "    cleaned = re.sub(pattern, '', question, flags=re.IGNORECASE).strip()\n",
    "    return cleaned if cleaned else question\n",
    "\n",
    "def preprocess_dataframe(df: pd.DataFrame, question_col: str) -> pd.DataFrame:\n",
    "    \"\"\"Apply cleaning to question column\"\"\"\n",
    "    df = df.copy()\n",
    "    df[question_col] = df[question_col].apply(clean_question)\n",
    "    return df\n",
    "\n",
    "print(\"âœ… Question preprocessing ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c2fddd4",
   "metadata": {},
   "source": [
    "## S3 Embeddings Cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccae5ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cache_key(text: str, model: str) -> str:\n",
    "    \"\"\"Generate S3 cache key for text\"\"\"\n",
    "    text_hash = hashlib.md5(text.encode()).hexdigest()[:12]\n",
    "    return f\"{S3_CACHE_PREFIX}/{model}/{text_hash}.pkl\"\n",
    "\n",
    "def load_embedding_from_s3(text: str, model: str) -> Optional[List[float]]:\n",
    "    \"\"\"Load cached embedding from S3\"\"\"\n",
    "    cache_key = get_cache_key(text, model)\n",
    "    local_path = f\"/tmp/{cache_key.split('/')[-1]}\"\n",
    "    \n",
    "    if download_from_s3(cache_key, local_path):\n",
    "        try:\n",
    "            with open(local_path, 'rb') as f:\n",
    "                embedding = pickle.load(f)\n",
    "            # Clean up temp file after reading\n",
    "            try:\n",
    "                os.unlink(local_path)\n",
    "            except:\n",
    "                pass\n",
    "            return embedding\n",
    "        except Exception as e:\n",
    "            # Clean up corrupted cache file\n",
    "            try:\n",
    "                os.unlink(local_path)\n",
    "            except:\n",
    "                pass\n",
    "    return None\n",
    "\n",
    "def save_embedding_to_s3(text: str, model: str, embedding: List[float]):\n",
    "    \"\"\"Save embedding to S3 cache with retry logic\"\"\"\n",
    "    cache_key = get_cache_key(text, model)\n",
    "    local_path = f\"/tmp/{cache_key.split('/')[-1]}\"\n",
    "    \n",
    "    try:\n",
    "        # Write to local temp file\n",
    "        with open(local_path, 'wb') as f:\n",
    "            pickle.dump(embedding, f)\n",
    "        \n",
    "        # Upload to S3 (with retry logic from upload_to_s3)\n",
    "        # Use public=False for cache files (no need for public access)\n",
    "        success = upload_to_s3(local_path, cache_key, public=False)\n",
    "        \n",
    "        # Clean up temp file\n",
    "        try:\n",
    "            os.unlink(local_path)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        return success\n",
    "    except Exception as e:\n",
    "        # Clean up on failure\n",
    "        try:\n",
    "            if os.path.exists(local_path):\n",
    "                os.unlink(local_path)\n",
    "        except:\n",
    "            pass\n",
    "        # Don't log - cache failures are expected and handled gracefully\n",
    "        return False\n",
    "\n",
    "print(\"âœ… S3 embeddings cache ready (with retry logic)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8237e2ac",
   "metadata": {},
   "source": [
    "## Embedding Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4938efe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings_batch(texts: List[str], model: str = EMBEDDING_MODEL, batch_size: int = 1000) -> List[List[float]]:\n",
    "    \"\"\"Generate embeddings with S3 caching\"\"\"\n",
    "    cleaned_texts = [clean_question(t) for t in texts]\n",
    "    embeddings = []\n",
    "    cache_hits = 0\n",
    "    api_calls = 0\n",
    "    \n",
    "    print(f\"ðŸ”„ Processing {len(cleaned_texts)} texts...\")\n",
    "    \n",
    "    for i in tqdm(range(0, len(cleaned_texts), batch_size), desc=\"Batches\"):\n",
    "        batch_texts = cleaned_texts[i:i+batch_size]\n",
    "        batch_embeddings = []\n",
    "        uncached_texts = []\n",
    "        uncached_indices = []\n",
    "        \n",
    "        # Check S3 cache\n",
    "        for j, text in enumerate(batch_texts):\n",
    "            cached = load_embedding_from_s3(text, model)\n",
    "            if cached:\n",
    "                batch_embeddings.append(cached)\n",
    "                cache_hits += 1\n",
    "            else:\n",
    "                batch_embeddings.append(None)\n",
    "                uncached_texts.append(text)\n",
    "                uncached_indices.append(j)\n",
    "        \n",
    "        # Generate uncached embeddings\n",
    "        if uncached_texts:\n",
    "            try:\n",
    "                response = client.embeddings.create(model=model, input=uncached_texts)\n",
    "                new_embeddings = [d.embedding for d in response.data]\n",
    "                api_calls += len(uncached_texts)\n",
    "                \n",
    "                for idx, emb in zip(uncached_indices, new_embeddings):\n",
    "                    batch_embeddings[idx] = emb\n",
    "                    save_embedding_to_s3(batch_texts[idx], model, emb)\n",
    "            except Exception as e:\n",
    "                error_logger.log_error(\"Embeddings\", f\"Batch failed\", e)\n",
    "                for idx in uncached_indices:\n",
    "                    batch_embeddings[idx] = [0.0] * EMBEDDING_DIMENSIONS\n",
    "        \n",
    "        embeddings.extend(batch_embeddings)\n",
    "    \n",
    "    print(f\"âœ… Complete! Cache: {cache_hits}/{len(embeddings)} ({cache_hits/len(embeddings)*100:.1f}%), API: {api_calls}\")\n",
    "    return embeddings\n",
    "\n",
    "print(\"âœ… Embedding generation ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8868d0ba",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f68989f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load topics from Google Sheets\n",
    "print(\"ðŸ“Š Loading topics from Google Sheets...\")\n",
    "topics_df = read_topics_from_google_sheets(GOOGLE_SHEETS_URL)\n",
    "topics_df = preprocess_dataframe(topics_df, 'question')\n",
    "\n",
    "# Upload Langfuse CSV\n",
    "if IN_COLAB:\n",
    "    from google.colab import files\n",
    "    print(\"\\nðŸ“‚ Upload Langfuse CSV:\")\n",
    "    uploaded = files.upload()\n",
    "    langfuse_filename = list(uploaded.keys())[0]\n",
    "else:\n",
    "    langfuse_filename = \"langfuse_traces_10_08_25.csv\"\n",
    "\n",
    "# Load and clean Langfuse data\n",
    "print(f\"\\nðŸ“Š Loading Langfuse data from {langfuse_filename}...\")\n",
    "langfuse_df = pd.read_csv(langfuse_filename)\n",
    "langfuse_clean = clean_langfuse_data(langfuse_df)\n",
    "\n",
    "# Create questions dataframe\n",
    "questions_df = pd.DataFrame({'question': langfuse_clean['input'].tolist()})\n",
    "questions_df = preprocess_dataframe(questions_df, 'question')\n",
    "\n",
    "print(f\"\\nðŸ“Š DATA LOADED:\")\n",
    "print(f\"   Topics: {len(topics_df)} ({topics_df['topic'].nunique()} unique)\")\n",
    "print(f\"   Questions: {len(questions_df)}\")\n",
    "print(f\"   Errors: {error_logger.get_summary()['total_errors']}\")\n",
    "print(f\"   Dropped rows: {error_logger.get_summary()['total_dropped_rows']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ef9941",
   "metadata": {},
   "source": [
    "## Prepare Evaluation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c347fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "if EVAL_MODE == \"sample\":\n",
    "    eval_df = questions_df.sample(n=min(SAMPLE_SIZE, len(questions_df)), random_state=RANDOM_SEED).copy()\n",
    "    print(f\"ðŸ“ Sample mode: {len(eval_df)} questions\")\n",
    "else:\n",
    "    eval_df = questions_df.copy()\n",
    "    print(f\"ðŸ“ Full mode: {len(eval_df)} questions\")\n",
    "\n",
    "# Cost estimate\n",
    "total_tokens = (len(topics_df) + len(eval_df)) * 50\n",
    "embedding_cost = (total_tokens / 1_000_000) * 0.02\n",
    "print(f\"ðŸ’° Estimated cost: ${embedding_cost:.4f} (embeddings only)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e251a632",
   "metadata": {},
   "source": [
    "## Similarity Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adab14e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_by_similarity(questions_df: pd.DataFrame, topics_df: pd.DataFrame, threshold: float) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"Classify questions by similarity to existing topics\"\"\"\n",
    "    \n",
    "    print(f\"\\nðŸŽ¯ Similarity Classification (threshold: {threshold})\")\n",
    "    \n",
    "    # Generate embeddings\n",
    "    print(f\"ðŸ“Š Generating topic embeddings...\")\n",
    "    topic_embeddings = get_embeddings_batch(topics_df['question'].tolist())\n",
    "    topics_df = topics_df.copy()\n",
    "    topics_df['embedding'] = topic_embeddings\n",
    "    \n",
    "    print(f\"ðŸ“Š Generating question embeddings...\")\n",
    "    question_embeddings = get_embeddings_batch(questions_df['question'].tolist())\n",
    "    \n",
    "    # Classify\n",
    "    similar = []\n",
    "    remaining = []\n",
    "    \n",
    "    print(f\"ðŸ” Classifying {len(questions_df)} questions...\")\n",
    "    for question, q_emb in tqdm(zip(questions_df['question'], question_embeddings), total=len(questions_df)):\n",
    "        if not q_emb or len(q_emb) != EMBEDDING_DIMENSIONS:\n",
    "            remaining.append({'question': question, 'embedding': [0.0]*EMBEDDING_DIMENSIONS})\n",
    "            continue\n",
    "        \n",
    "        best_sim = 0\n",
    "        best_match = None\n",
    "        \n",
    "        for _, topic_row in topics_df.iterrows():\n",
    "            t_emb = topic_row['embedding']\n",
    "            if t_emb and len(t_emb) == EMBEDDING_DIMENSIONS:\n",
    "                sim = 1 - cosine(q_emb, t_emb)\n",
    "                if sim > best_sim:\n",
    "                    best_sim = sim\n",
    "                    best_match = topic_row\n",
    "        \n",
    "        if best_sim >= threshold and best_match is not None:\n",
    "            similar.append({\n",
    "                'question': question,\n",
    "                'matched_topic': best_match['topic'],\n",
    "                'matched_subtopic': best_match['subtopic'],\n",
    "                'similarity_score': best_sim\n",
    "            })\n",
    "        else:\n",
    "            remaining.append({'question': question, 'embedding': q_emb})\n",
    "    \n",
    "    similar_df = pd.DataFrame(similar)\n",
    "    remaining_df = pd.DataFrame(remaining)\n",
    "    \n",
    "    print(f\"\\nâœ… Classification complete:\")\n",
    "    print(f\"   Similar: {len(similar_df)} ({len(similar_df)/len(questions_df)*100:.1f}%)\")\n",
    "    print(f\"   Remaining: {len(remaining_df)} ({len(remaining_df)/len(questions_df)*100:.1f}%)\")\n",
    "    \n",
    "    return similar_df, remaining_df\n",
    "\n",
    "similar_df, remaining_df = classify_by_similarity(eval_df, topics_df, SIMILARITY_THRESHOLD)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d352ac43",
   "metadata": {},
   "source": [
    "## Clustering for New Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b483e59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "clustered_df = None\n",
    "topic_model = None\n",
    "\n",
    "if len(remaining_df) > 0:\n",
    "    print(f\"\\nðŸŽ¯ Clustering {len(remaining_df)} remaining questions...\")\n",
    "    \n",
    "    from umap import UMAP\n",
    "    from hdbscan import HDBSCAN\n",
    "    from bertopic import BERTopic\n",
    "    \n",
    "    embeddings = np.array(remaining_df['embedding'].tolist())\n",
    "    \n",
    "    # UMAP\n",
    "    print(f\"ðŸ”„ UMAP reduction to {UMAP_N_COMPONENTS} dimensions...\")\n",
    "    umap_model = UMAP(n_components=UMAP_N_COMPONENTS, min_dist=0.0, metric='cosine', random_state=RANDOM_SEED)\n",
    "    reduced = umap_model.fit_transform(embeddings)\n",
    "    \n",
    "    # HDBSCAN\n",
    "    print(f\"ðŸ”„ HDBSCAN clustering (min_size={HDBSCAN_MIN_CLUSTER_SIZE})...\")\n",
    "    hdbscan_model = HDBSCAN(min_cluster_size=HDBSCAN_MIN_CLUSTER_SIZE, metric=\"euclidean\", cluster_selection_method=\"eom\")\n",
    "    clusters = hdbscan_model.fit_predict(reduced)\n",
    "    \n",
    "    n_clusters = len([c for c in np.unique(clusters) if c != -1])\n",
    "    n_noise = sum(clusters == -1)\n",
    "    print(f\"âœ… Found {n_clusters} clusters, {n_noise} noise points\")\n",
    "    \n",
    "    if n_clusters > 0:\n",
    "        # BERTopic\n",
    "        topic_model = BERTopic(embedding_model=None, umap_model=umap_model, hdbscan_model=hdbscan_model, verbose=False)\n",
    "        topics, probs = topic_model.fit_transform(remaining_df['question'].tolist(), embeddings)\n",
    "        \n",
    "        clustered_df = remaining_df.copy()\n",
    "        clustered_df['cluster_id'] = clusters\n",
    "        clustered_df['topic_id'] = topics\n",
    "        clustered_df = clustered_df[clustered_df['cluster_id'] != -1]\n",
    "        \n",
    "        print(f\"âœ… Clustered {len(clustered_df)} questions into {n_clusters} topics\")\n",
    "else:\n",
    "    print(\"\\nâœ… All questions matched existing topics - no clustering needed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14cf39bc",
   "metadata": {},
   "source": [
    "## Generate Topic Names with GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f310a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_names = {}\n",
    "\n",
    "if clustered_df is not None and len(clustered_df) > 0:\n",
    "    print(f\"\\nðŸ¤– Generating topic names with {GPT_MODEL}...\")\n",
    "    \n",
    "    async def generate_topic_name(questions: List[str]) -> str:\n",
    "        sample = questions[:10]\n",
    "        prompt = f\"\"\"Generate a concise topic name (2-8 words) for these student questions:\n",
    "\n",
    "{chr(10).join([f'- {q}' for q in sample])}\n",
    "\n",
    "Return ONLY the topic name, no explanation.\"\"\"\n",
    "        \n",
    "        try:\n",
    "            response = await async_client.chat.completions.create(\n",
    "                model=GPT_MODEL,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                max_tokens=100\n",
    "            )\n",
    "            return response.choices[0].message.content.strip().strip('\"')\n",
    "        except Exception as e:\n",
    "            error_logger.log_error(\"TopicNaming\", \"GPT failed\", e)\n",
    "            return \"Unnamed Topic\"\n",
    "    \n",
    "    async def process_all_clusters():\n",
    "        tasks = []\n",
    "        cluster_ids = []\n",
    "        for cluster_id, group in clustered_df.groupby('cluster_id'):\n",
    "            tasks.append(generate_topic_name(group['question'].tolist()))\n",
    "            cluster_ids.append(cluster_id)\n",
    "        \n",
    "        names = await asyncio.gather(*tasks)\n",
    "        return dict(zip(cluster_ids, names))\n",
    "    \n",
    "    topic_names = await process_all_clusters()\n",
    "    clustered_df['topic_name'] = clustered_df['cluster_id'].map(topic_names)\n",
    "    \n",
    "    print(f\"âœ… Generated {len(topic_names)} topic names\")\n",
    "    for cid, name in list(topic_names.items())[:5]:\n",
    "        count = len(clustered_df[clustered_df['cluster_id'] == cid])\n",
    "        print(f\"   {name} ({count} questions)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f30620d7",
   "metadata": {},
   "source": [
    "## Generate Output Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc0920e",
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "print(f\"\\nðŸ“ Generating output files...\")\n",
    "\n",
    "# File 1: Similar questions\n",
    "file1 = f\"similar_questions_{timestamp}.parquet\"\n",
    "if len(similar_df) > 0:\n",
    "    output1 = similar_df[['question', 'matched_topic', 'matched_subtopic', 'similarity_score']].copy()\n",
    "    output1.columns = ['question', 'existing_topic', 'existing_subtopic', 'similarity_score']\n",
    "    output1 = output1.sort_values('similarity_score', ascending=False)\n",
    "else:\n",
    "    output1 = pd.DataFrame(columns=['question', 'existing_topic', 'existing_subtopic', 'similarity_score'])\n",
    "\n",
    "# Add metadata\n",
    "output1.attrs['metadata'] = {\n",
    "    'timestamp': timestamp,\n",
    "    'threshold': SIMILARITY_THRESHOLD,\n",
    "    'total_questions': len(output1),\n",
    "    'default_visible_columns': ['question', 'existing_topic', 'existing_subtopic', 'similarity_score']\n",
    "}\n",
    "output1.to_parquet(file1)\n",
    "print(f\"âœ… {file1}: {len(output1)} rows\")\n",
    "\n",
    "# File 2: New topics\n",
    "file2 = f\"new_topics_{timestamp}.parquet\"\n",
    "if clustered_df is not None and len(clustered_df) > 0:\n",
    "    cluster_summary = clustered_df.groupby('cluster_id').agg({\n",
    "        'topic_name': 'first',\n",
    "        'question': ['first', 'count']\n",
    "    }).reset_index()\n",
    "    cluster_summary.columns = ['cluster_id', 'topic_name', 'representative_question', 'question_count']\n",
    "    output2 = cluster_summary[['topic_name', 'representative_question', 'question_count']].sort_values('question_count', ascending=False)\n",
    "else:\n",
    "    output2 = pd.DataFrame(columns=['topic_name', 'representative_question', 'question_count'])\n",
    "\n",
    "output2.attrs['metadata'] = {\n",
    "    'timestamp': timestamp,\n",
    "    'total_topics': len(output2),\n",
    "    'default_visible_columns': ['topic_name', 'representative_question', 'question_count']\n",
    "}\n",
    "output2.to_parquet(file2)\n",
    "print(f\"âœ… {file2}: {len(output2)} rows\")\n",
    "\n",
    "# File 3: All questions review\n",
    "file3 = f\"pathway_questions_review_{timestamp}.parquet\"\n",
    "review_data = []\n",
    "\n",
    "if len(similar_df) > 0:\n",
    "    for _, row in similar_df.iterrows():\n",
    "        review_data.append({\n",
    "            'question': row['question'],\n",
    "            'topic_name': f\"{row['matched_topic']} | {row['matched_subtopic']}\",\n",
    "            'classification': 'existing',\n",
    "            'confidence': row['similarity_score']\n",
    "        })\n",
    "\n",
    "if clustered_df is not None and len(clustered_df) > 0:\n",
    "    for _, row in clustered_df.iterrows():\n",
    "        review_data.append({\n",
    "            'question': row['question'],\n",
    "            'topic_name': row['topic_name'],\n",
    "            'classification': 'new',\n",
    "            'confidence': 0.5\n",
    "        })\n",
    "\n",
    "if len(remaining_df) > len(clustered_df) if clustered_df is not None else len(remaining_df) > 0:\n",
    "    clustered_questions = set(clustered_df['question']) if clustered_df is not None else set()\n",
    "    for _, row in remaining_df.iterrows():\n",
    "        if row['question'] not in clustered_questions:\n",
    "            review_data.append({\n",
    "                'question': row['question'],\n",
    "                'topic_name': 'Other',\n",
    "                'classification': 'uncategorized',\n",
    "                'confidence': 0.0\n",
    "            })\n",
    "\n",
    "output3 = pd.DataFrame(review_data)\n",
    "output3.attrs['metadata'] = {\n",
    "    'timestamp': timestamp,\n",
    "    'total_questions': len(output3),\n",
    "    'default_visible_columns': ['question', 'topic_name', 'classification', 'confidence']\n",
    "}\n",
    "output3.to_parquet(file3)\n",
    "print(f\"âœ… {file3}: {len(output3)} rows\")\n",
    "\n",
    "# File 4: Topic distribution analytics\n",
    "file4 = f\"topic_distribution_{timestamp}.parquet\"\n",
    "topic_dist = output3.groupby(['topic_name', 'classification']).size().reset_index(name='count')\n",
    "topic_dist = topic_dist.sort_values('count', ascending=False)\n",
    "topic_dist.attrs['metadata'] = {'timestamp': timestamp, 'purpose': 'streamlit_analytics'}\n",
    "topic_dist.to_parquet(file4)\n",
    "print(f\"âœ… {file4}: {len(topic_dist)} rows\")\n",
    "\n",
    "# File 5: Error log\n",
    "file5 = f\"error_log_{timestamp}.json\"\n",
    "error_logger.save_to_file(file5)\n",
    "\n",
    "output_files = [file1, file2, file3, file4, file5]\n",
    "print(f\"\\nâœ… Generated {len(output_files)} output files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a7625c6",
   "metadata": {},
   "source": [
    "## Upload to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e9af3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nâ˜ï¸  Uploading to S3...\")\n",
    "\n",
    "# Delete old files\n",
    "delete_s3_folder(S3_OUTPUT_PREFIX)\n",
    "\n",
    "# Upload new files\n",
    "uploaded = []\n",
    "for filepath in output_files:\n",
    "    s3_key = f\"{S3_OUTPUT_PREFIX}/{filepath}\"\n",
    "    if upload_to_s3(filepath, s3_key):\n",
    "        uploaded.append(f\"https://{S3_BUCKET}.s3.amazonaws.com/{s3_key}\")\n",
    "\n",
    "print(f\"\\nâœ… Uploaded {len(uploaded)} files to S3:\")\n",
    "for url in uploaded:\n",
    "    print(f\"   {url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7865c4b",
   "metadata": {},
   "source": [
    "## Analysis & Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d64886",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "fig.suptitle('Hybrid Topic Discovery Analysis', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Processing pipeline\n",
    "pipeline = ['Total', 'Similar', 'New Topics', 'Uncategorized']\n",
    "counts = [\n",
    "    len(eval_df),\n",
    "    len(similar_df),\n",
    "    len(clustered_df) if clustered_df is not None else 0,\n",
    "    len(eval_df) - len(similar_df) - (len(clustered_df) if clustered_df is not None else 0)\n",
    "]\n",
    "axes[0,0].bar(pipeline, counts, color=['lightblue', 'lightgreen', 'orange', 'lightcoral'])\n",
    "axes[0,0].set_title('Processing Results')\n",
    "axes[0,0].set_ylabel('Questions')\n",
    "for i, (label, count) in enumerate(zip(pipeline, counts)):\n",
    "    axes[0,0].text(i, count + max(counts)*0.01, f\"{count}\\n({count/len(eval_df)*100:.1f}%)\", ha='center')\n",
    "\n",
    "# 2. Similarity distribution\n",
    "if len(similar_df) > 0:\n",
    "    axes[0,1].hist(similar_df['similarity_score'], bins=20, alpha=0.7, color='lightgreen', edgecolor='black')\n",
    "    axes[0,1].axvline(SIMILARITY_THRESHOLD, color='red', linestyle='--', label=f'Threshold: {SIMILARITY_THRESHOLD}')\n",
    "    axes[0,1].set_xlabel('Similarity Score')\n",
    "    axes[0,1].set_ylabel('Count')\n",
    "    axes[0,1].set_title('Similarity Distribution')\n",
    "    axes[0,1].legend()\n",
    "else:\n",
    "    axes[0,1].text(0.5, 0.5, 'No similar questions', ha='center', va='center', transform=axes[0,1].transAxes)\n",
    "    axes[0,1].set_title('Similarity Distribution')\n",
    "\n",
    "# 3. Cluster sizes\n",
    "if clustered_df is not None and len(clustered_df) > 0:\n",
    "    cluster_sizes = clustered_df['cluster_id'].value_counts().values\n",
    "    axes[1,0].hist(cluster_sizes, bins=min(20, len(cluster_sizes)), alpha=0.7, color='orange', edgecolor='black')\n",
    "    axes[1,0].set_xlabel('Cluster Size')\n",
    "    axes[1,0].set_ylabel('Count')\n",
    "    axes[1,0].set_title('New Topic Sizes')\n",
    "else:\n",
    "    axes[1,0].text(0.5, 0.5, 'No clusters', ha='center', va='center', transform=axes[1,0].transAxes)\n",
    "    axes[1,0].set_title('New Topic Sizes')\n",
    "\n",
    "# 4. Topic distribution pie\n",
    "pie_data = output3['classification'].value_counts()\n",
    "if len(pie_data) > 0:\n",
    "    axes[1,1].pie(pie_data.values, labels=pie_data.index, autopct='%1.1f%%', startangle=90)\n",
    "    axes[1,1].set_title('Classification Distribution')\n",
    "else:\n",
    "    axes[1,1].text(0.5, 0.5, 'No data', ha='center', va='center', transform=axes[1,1].transAxes)\n",
    "    axes[1,1].set_title('Classification Distribution')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ“Š SUMMARY:\")\n",
    "print(f\"   Total processed: {len(eval_df)}\")\n",
    "print(f\"   Similar to existing: {len(similar_df)} ({len(similar_df)/len(eval_df)*100:.1f}%)\")\n",
    "print(f\"   New topics: {len(topic_names)}\")\n",
    "print(f\"   Errors: {error_logger.get_summary()['total_errors']}\")\n",
    "print(f\"   Warnings: {error_logger.get_summary()['total_warnings']}\")\n",
    "print(f\"\\nâœ… COMPLETE! Files uploaded to S3.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
