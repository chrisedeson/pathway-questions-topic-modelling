{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66de1f2f",
   "metadata": {},
   "source": [
    "# üîß Fix GPT-5 Configuration\n",
    "\n",
    "**Issue**: GPT-5 models (gpt-5-nano, gpt-5-mini) use different API parameters than GPT-4:\n",
    "- ‚ùå Use `max_completion_tokens` instead of `max_tokens`\n",
    "- ‚ùå Do NOT include `temperature` parameter (GPT-5 always uses temperature=1)\n",
    "- ‚úÖ Must include proper system message and formatting\n",
    "\n",
    "This cell contains the corrected code from `insights.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca4a781",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CORRECTED CODE - Copy this to replace the faulty cell in your main notebook\n",
    "\n",
    "topic_names = {}\n",
    "\n",
    "if clustered_df is not None and len(clustered_df) > 0:\n",
    "    print(f\"\\nü§ñ Generating topic names with {GPT_MODEL}...\")\n",
    "    \n",
    "    async def generate_topic_name(questions: List[str], keywords: str = \"\") -> str:\n",
    "        \"\"\"Generate a topic name using GPT-5 for a cluster of questions\"\"\"\n",
    "        \n",
    "        # Limit to top 10 questions for context (like insights)\n",
    "        sample_questions = questions[:10]\n",
    "        questions_text = \"\\n\".join([f\"- {q}\" for q in sample_questions])\n",
    "\n",
    "        prompt = f\"\"\"\n",
    "Based on the following student questions and keywords, generate a concise, descriptive topic name.\n",
    "\n",
    "QUESTIONS:\n",
    "{questions_text}\n",
    "\n",
    "KEYWORDS: {keywords}\n",
    "\n",
    "Instructions:\n",
    "- Your answer must be ONLY the topic name (2‚Äì8 words), no extra text.\n",
    "- It should clearly describe the shared theme of the questions.\n",
    "- Avoid generic labels like \"General Questions\" or \"Miscellaneous.\"\n",
    "- Do not include \"Topic name:\" or quotation marks.\n",
    "- Use simple, natural English that sounds clear to a student or teacher.\n",
    "\n",
    "Example:\n",
    "Questions:\n",
    "- When does registration open?\n",
    "- What are the fall 2025 enrollment deadlines?\n",
    "Keywords: registration, deadlines\n",
    "\n",
    "Topic name: Fall 2025 Registration Deadlines\n",
    "\n",
    "Now generate the topic name for the questions above:\n",
    "\"\"\"\n",
    "\n",
    "        try:\n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": \"You are an expert at creating clear, descriptive topic names for student question categories.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ]\n",
    "            \n",
    "            # GPT-5 specific configuration (NO temperature parameter!)\n",
    "            response = await async_client.chat.completions.create(\n",
    "                model=GPT_MODEL,\n",
    "                messages=messages,\n",
    "                max_completion_tokens=1000  # Use max_completion_tokens for GPT-5, not max_tokens\n",
    "            )\n",
    "            \n",
    "            topic_name = response.choices[0].message.content.strip()\n",
    "            \n",
    "            # Clean up the response\n",
    "            topic_name = topic_name.replace(\"Topic name:\", \"\").strip()\n",
    "            topic_name = topic_name.strip('\\\"\\'')\n",
    "            \n",
    "            if not topic_name:\n",
    "                topic_name = f\"Topic: {keywords[:50]}\" if keywords else f\"Question Group {hash(str(questions[:3])) % 1000}\"\n",
    "            \n",
    "            return topic_name\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_logger.log_error(\"TopicNaming\", f\"GPT failed: {str(e)}\", e)\n",
    "            # Fallback to keyword-based name\n",
    "            fallback_name = f\"Topic: {keywords[:50]}\" if keywords else f\"Question Group {hash(str(questions[:3])) % 1000}\"\n",
    "            return fallback_name\n",
    "    \n",
    "    async def process_all_clusters():\n",
    "        tasks = []\n",
    "        cluster_ids = []\n",
    "        \n",
    "        for cluster_id, group in clustered_df.groupby('cluster_id'):\n",
    "            questions = group['question'].tolist()\n",
    "            # Extract keywords from BERTopic if available\n",
    "            keywords = group['topic_keywords'].iloc[0] if 'topic_keywords' in group.columns else \"\"\n",
    "            \n",
    "            tasks.append(generate_topic_name(questions, keywords))\n",
    "            cluster_ids.append(cluster_id)\n",
    "        \n",
    "        names = await asyncio.gather(*tasks)\n",
    "        return dict(zip(cluster_ids, names))\n",
    "    \n",
    "    topic_names = await process_all_clusters()\n",
    "    clustered_df['topic_name'] = clustered_df['cluster_id'].map(topic_names)\n",
    "    \n",
    "    print(f\"‚úÖ Generated {len(topic_names)} topic names\")\n",
    "    for cid, name in list(topic_names.items())[:5]:\n",
    "        count = len(clustered_df[clustered_df['cluster_id'] == cid])\n",
    "        print(f\"   {name} ({count} questions)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44e17cf",
   "metadata": {},
   "source": [
    "## üìã Key Changes from Original\n",
    "\n",
    "### ‚ùå What was wrong:\n",
    "```python\n",
    "response = await async_client.chat.completions.create(\n",
    "    model=GPT_MODEL,\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "    max_tokens=100  # ‚ùå Wrong parameter for GPT-5\n",
    ")\n",
    "```\n",
    "\n",
    "### ‚úÖ What's correct:\n",
    "```python\n",
    "response = await async_client.chat.completions.create(\n",
    "    model=GPT_MODEL,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"...\"},  # ‚úÖ System message\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ],\n",
    "    max_completion_tokens=1000  # ‚úÖ Correct parameter for GPT-5\n",
    "    # ‚úÖ NO temperature parameter - GPT-5 always uses 1.0\n",
    ")\n",
    "```\n",
    "\n",
    "## üîç Other Improvements:\n",
    "\n",
    "1. **Better prompt** - More detailed instructions and examples\n",
    "2. **System message** - Establishes GPT's role as topic naming expert\n",
    "3. **Keywords integration** - Uses BERTopic keywords if available\n",
    "4. **Better fallback** - Uses keywords for unnamed topics\n",
    "5. **Response cleaning** - Removes \"Topic name:\" prefix and quotes\n",
    "\n",
    "## üöÄ How to Use:\n",
    "\n",
    "1. Copy the code from the cell above\n",
    "2. In your main notebook, find the \"Generate Topic Names with GPT\" section\n",
    "3. Replace the entire cell with this corrected version\n",
    "4. Run the notebook again\n",
    "\n",
    "The GPT-5 errors should disappear! ‚ú®"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf557d88",
   "metadata": {},
   "source": [
    "## üîß Fix S3 Upload Failures\n",
    "\n",
    "**Issue**: Output files are failing to upload even with retry logic.\n",
    "\n",
    "This is likely due to one of these issues:\n",
    "1. Missing `public=True` parameter in upload call\n",
    "2. Files don't exist in current directory\n",
    "3. Permission issue with `PutObjectAcl` (public-read ACL)\n",
    "4. Network/region configuration issue\n",
    "\n",
    "Let's add debugging and fix the upload code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43934ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTION 1: Fix with better error handling and file verification\n",
    "\n",
    "print(f\"\\n‚òÅÔ∏è  Uploading to S3...\")\n",
    "\n",
    "# Delete old files (optional - comment out if you want to keep history)\n",
    "try:\n",
    "    delete_s3_folder(S3_OUTPUT_PREFIX)\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  Could not delete old files: {e}\")\n",
    "\n",
    "# Upload new files with verification\n",
    "uploaded = []\n",
    "failed = []\n",
    "\n",
    "for filepath in output_files:\n",
    "    # Check if file exists\n",
    "    if not os.path.exists(filepath):\n",
    "        print(f\"‚ùå File not found: {filepath}\")\n",
    "        failed.append(filepath)\n",
    "        continue\n",
    "    \n",
    "    # Get file size for debugging\n",
    "    file_size = os.path.getsize(filepath)\n",
    "    print(f\"üì§ Uploading {filepath} ({file_size:,} bytes)...\")\n",
    "    \n",
    "    s3_key = f\"{S3_OUTPUT_PREFIX}/{filepath}\"\n",
    "    \n",
    "    try:\n",
    "        # Explicitly pass public=True for output files\n",
    "        if upload_to_s3(filepath, s3_key, public=True):\n",
    "            url = f\"https://{S3_BUCKET}.s3.amazonaws.com/{s3_key}\"\n",
    "            uploaded.append(url)\n",
    "            print(f\"   ‚úÖ Success: {url}\")\n",
    "        else:\n",
    "            failed.append(filepath)\n",
    "            print(f\"   ‚ùå Failed: {filepath}\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Exception: {str(e)}\")\n",
    "        failed.append(filepath)\n",
    "\n",
    "print(f\"\\nüìä UPLOAD SUMMARY:\")\n",
    "print(f\"   ‚úÖ Successful: {len(uploaded)}/{len(output_files)}\")\n",
    "print(f\"   ‚ùå Failed: {len(failed)}/{len(output_files)}\")\n",
    "\n",
    "if uploaded:\n",
    "    print(f\"\\n‚úÖ Uploaded files:\")\n",
    "    for url in uploaded:\n",
    "        print(f\"   {url}\")\n",
    "\n",
    "if failed:\n",
    "    print(f\"\\n‚ùå Failed files:\")\n",
    "    for f in failed:\n",
    "        print(f\"   {f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa53059",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTION 2: Skip ACL if you don't have PutObjectAcl permission\n",
    "# Use this if your diagnostic showed you can upload without ACL but not with public-read\n",
    "\n",
    "print(f\"\\n‚òÅÔ∏è  Uploading to S3 (without public ACL)...\")\n",
    "\n",
    "# Delete old files\n",
    "try:\n",
    "    delete_s3_folder(S3_OUTPUT_PREFIX)\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  Could not delete old files: {e}\")\n",
    "\n",
    "# Upload new files WITHOUT public-read ACL\n",
    "uploaded = []\n",
    "failed = []\n",
    "\n",
    "for filepath in output_files:\n",
    "    if not os.path.exists(filepath):\n",
    "        print(f\"‚ùå File not found: {filepath}\")\n",
    "        failed.append(filepath)\n",
    "        continue\n",
    "    \n",
    "    file_size = os.path.getsize(filepath)\n",
    "    print(f\"üì§ Uploading {filepath} ({file_size:,} bytes)...\")\n",
    "    \n",
    "    s3_key = f\"{S3_OUTPUT_PREFIX}/{filepath}\"\n",
    "    \n",
    "    try:\n",
    "        # Use public=False to skip ACL (if you don't have PutObjectAcl permission)\n",
    "        if upload_to_s3(filepath, s3_key, public=False):\n",
    "            # Note: URL won't be publicly accessible without ACL\n",
    "            url = f\"s3://{S3_BUCKET}/{s3_key}\"\n",
    "            uploaded.append(url)\n",
    "            print(f\"   ‚úÖ Success: {url}\")\n",
    "        else:\n",
    "            failed.append(filepath)\n",
    "            print(f\"   ‚ùå Failed: {filepath}\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Exception: {str(e)}\")\n",
    "        failed.append(filepath)\n",
    "\n",
    "print(f\"\\nüìä UPLOAD SUMMARY:\")\n",
    "print(f\"   ‚úÖ Successful: {len(uploaded)}/{len(output_files)}\")\n",
    "print(f\"   ‚ùå Failed: {len(failed)}/{len(output_files)}\")\n",
    "\n",
    "if uploaded:\n",
    "    print(f\"\\n‚úÖ Uploaded files (private - not publicly accessible):\")\n",
    "    for url in uploaded:\n",
    "        print(f\"   {url}\")\n",
    "    print(f\"\\nüí° TIP: Files are uploaded but not public. Your Streamlit app can access them with AWS credentials.\")\n",
    "\n",
    "if failed:\n",
    "    print(f\"\\n‚ùå Failed files:\")\n",
    "    for f in failed:\n",
    "        print(f\"   {f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff48bc4a",
   "metadata": {},
   "source": [
    "## üîç Root Cause Analysis\n",
    "\n",
    "Based on your diagnostic test results:\n",
    "\n",
    "### ‚úÖ What Works:\n",
    "- Test 3 & 4: Small file uploads to S3 succeed\n",
    "- Network connectivity is fine\n",
    "- Credentials are valid\n",
    "\n",
    "### ‚ùå What's Failing:\n",
    "- **ALL output files** fail to upload (not just cache files)\n",
    "- Retry logic exhausts all 5 attempts\n",
    "- Different from cache upload failures (which were silent)\n",
    "\n",
    "### üéØ Most Likely Causes:\n",
    "\n",
    "**1. Missing `s3:PutObjectAcl` Permission** (Most Likely)\n",
    "- Your IAM user can upload files (`s3:PutObject` works)\n",
    "- But cannot set ACL to `public-read` (`s3:PutObjectAcl` missing)\n",
    "- Solution: Use `public=False` or ask admin to add permission\n",
    "\n",
    "**2. File Path Issues**\n",
    "- Files might be in `/tmp/` but code looks in current directory\n",
    "- Check with `!ls -la *.parquet *.json`\n",
    "\n",
    "**3. File Size Issues**\n",
    "- Small test files (110-152 bytes) worked\n",
    "- Large parquet files might timeout\n",
    "- Unlikely since retry logic should handle this\n",
    "\n",
    "### üí° Recommended Solutions:\n",
    "\n",
    "**Option A** (Quick Fix): Use `public=False`\n",
    "- Files upload successfully but aren't publicly accessible\n",
    "- Streamlit app can still read them with AWS credentials\n",
    "- No IAM permission changes needed\n",
    "\n",
    "**Option B** (Proper Fix): Request IAM Permission\n",
    "- Ask your AWS admin to add `s3:PutObjectAcl` permission\n",
    "- Allows public-read ACL for sharing files\n",
    "- Enables public URLs for Streamlit\n",
    "\n",
    "**Option C** (Workaround): Bucket Policy\n",
    "- Set bucket-level policy to make all files public by default\n",
    "- No ACL needed per-file\n",
    "- Requires bucket admin access"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83828516",
   "metadata": {},
   "source": [
    "## üìã Quick Summary\n",
    "\n",
    "### Two Issues Fixed in This Notebook:\n",
    "\n",
    "#### 1Ô∏è‚É£ **GPT-5 Topic Naming** ‚ùå‚Üí‚úÖ\n",
    "- **Problem**: Used wrong API parameters (`max_tokens` vs `max_completion_tokens`)\n",
    "- **Fix**: Use corrected code from first cell (matches insights.ipynb)\n",
    "- **Status**: Ready to apply\n",
    "\n",
    "#### 2Ô∏è‚É£ **S3 Upload Failures** ‚ùå‚Üí‚ö†Ô∏è\n",
    "- **Problem**: Missing `s3:PutObjectAcl` IAM permission\n",
    "- **Options**:\n",
    "  - **Quick**: Use `public=False` (Option 2 code)\n",
    "  - **Proper**: Request IAM permission from AWS admin\n",
    "- **Status**: Choose your approach and apply code\n",
    "\n",
    "### üöÄ Next Steps:\n",
    "\n",
    "1. **Fix GPT-5**: Replace topic naming cell in main notebook with corrected version\n",
    "2. **Fix Uploads**: Choose Option 1 or Option 2 based on your needs\n",
    "3. **Test**: Run the full notebook end-to-end\n",
    "4. **Verify**: Check S3 bucket for uploaded files\n",
    "\n",
    "Your notebook should work perfectly after these changes! üéâ"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
