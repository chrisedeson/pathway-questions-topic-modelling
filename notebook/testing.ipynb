{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3db8e761",
   "metadata": {},
   "source": [
    "# üîç S3 Diagnostic Testing Notebook\n",
    "Test S3 connectivity, permissions, and upload functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff74fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install boto3 python-dotenv -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484285b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import os\n",
    "import pickle\n",
    "import tempfile\n",
    "from datetime import datetime\n",
    "\n",
    "# Load credentials\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    AWS_ACCESS_KEY_ID = userdata.get('AWS_ACCESS_KEY_ID')\n",
    "    AWS_SECRET_ACCESS_KEY = userdata.get('AWS_SECRET_ACCESS_KEY')\n",
    "    print(\"‚úÖ Loaded credentials from Google Colab secrets\")\n",
    "except ImportError:\n",
    "    from dotenv import load_dotenv\n",
    "    load_dotenv()\n",
    "    AWS_ACCESS_KEY_ID = os.getenv('AWS_ACCESS_KEY_ID')\n",
    "    AWS_SECRET_ACCESS_KEY = os.getenv('AWS_SECRET_ACCESS_KEY')\n",
    "    print(\"‚úÖ Loaded credentials from .env file\")\n",
    "\n",
    "# Configuration\n",
    "S3_BUCKET = \"byupathway-public\"\n",
    "CACHE_PREFIX = \"embeddings-cache\"\n",
    "TEST_PREFIX = \"test-diagnostics\"\n",
    "\n",
    "print(f\"\\nüì¶ Target Bucket: {S3_BUCKET}\")\n",
    "print(f\"üóÇÔ∏è  Cache Prefix: {CACHE_PREFIX}\")\n",
    "print(f\"üß™ Test Prefix: {TEST_PREFIX}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83523354",
   "metadata": {},
   "source": [
    "## Test 1: AWS Credentials Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be967672",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TEST 1: AWS Credentials Validation\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "try:\n",
    "    # Create boto3 client\n",
    "    s3 = boto3.client(\n",
    "        's3',\n",
    "        aws_access_key_id=AWS_ACCESS_KEY_ID,\n",
    "        aws_secret_access_key=AWS_SECRET_ACCESS_KEY,\n",
    "        region_name='us-east-1'\n",
    "    )\n",
    "    \n",
    "    # Test credentials by listing buckets\n",
    "    response = s3.list_buckets()\n",
    "    print(\"‚úÖ AWS Credentials Valid\")\n",
    "    print(f\"‚úÖ Found {len(response['Buckets'])} accessible buckets\")\n",
    "    \n",
    "    # Check if target bucket exists\n",
    "    bucket_names = [b['Name'] for b in response['Buckets']]\n",
    "    if S3_BUCKET in bucket_names:\n",
    "        print(f\"‚úÖ Target bucket '{S3_BUCKET}' is accessible\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è  Target bucket '{S3_BUCKET}' not in your accessible buckets list\")\n",
    "        print(f\"   Your accessible buckets: {bucket_names}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Credentials Test Failed: {str(e)}\")\n",
    "    print(f\"   Error Type: {type(e).__name__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4afebf",
   "metadata": {},
   "source": [
    "## Test 2: Bucket Access & List Permissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9ba714",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TEST 2: Bucket Access & List Permissions\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "try:\n",
    "    # Try to list objects in the cache prefix\n",
    "    response = s3.list_objects_v2(\n",
    "        Bucket=S3_BUCKET,\n",
    "        Prefix=CACHE_PREFIX,\n",
    "        MaxKeys=10\n",
    "    )\n",
    "    \n",
    "    object_count = response.get('KeyCount', 0)\n",
    "    print(f\"‚úÖ Successfully listed objects in '{S3_BUCKET}/{CACHE_PREFIX}'\")\n",
    "    print(f\"‚úÖ Found {object_count} objects (showing max 10)\")\n",
    "    \n",
    "    if object_count > 0:\n",
    "        print(\"\\nSample objects:\")\n",
    "        for obj in response.get('Contents', [])[:3]:\n",
    "            print(f\"  - {obj['Key']} ({obj['Size']} bytes)\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå List Objects Failed: {str(e)}\")\n",
    "    print(f\"   Error Type: {type(e).__name__}\")\n",
    "    if hasattr(e, 'response'):\n",
    "        print(f\"   HTTP Status: {e.response.get('ResponseMetadata', {}).get('HTTPStatusCode')}\")\n",
    "        print(f\"   Error Code: {e.response.get('Error', {}).get('Code')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b3dfad0",
   "metadata": {},
   "source": [
    "## Test 3: Write/Upload Permissions (Test Prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d92fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TEST 3: Write/Upload Permissions (Test Prefix)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "try:\n",
    "    # Create a small test file\n",
    "    test_data = {\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "        'test': 'S3 upload diagnostic',\n",
    "        'values': [1, 2, 3, 4, 5]\n",
    "    }\n",
    "    \n",
    "    # Save to temp file\n",
    "    with tempfile.NamedTemporaryFile(mode='wb', suffix='.pkl', delete=False) as tmp:\n",
    "        pickle.dump(test_data, tmp)\n",
    "        tmp_path = tmp.name\n",
    "    \n",
    "    # Generate test key\n",
    "    test_key = f\"{TEST_PREFIX}/diagnostic_test_{datetime.now().strftime('%Y%m%d_%H%M%S')}.pkl\"\n",
    "    \n",
    "    print(f\"üì§ Attempting upload to: s3://{S3_BUCKET}/{test_key}\")\n",
    "    \n",
    "    # Upload to S3\n",
    "    s3.upload_file(\n",
    "        tmp_path,\n",
    "        S3_BUCKET,\n",
    "        test_key\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ Upload successful!\")\n",
    "    \n",
    "    # Verify the upload\n",
    "    response = s3.head_object(Bucket=S3_BUCKET, Key=test_key)\n",
    "    print(f\"‚úÖ File verified on S3\")\n",
    "    print(f\"   Size: {response['ContentLength']} bytes\")\n",
    "    print(f\"   Last Modified: {response['LastModified']}\")\n",
    "    \n",
    "    # Clean up local file\n",
    "    os.unlink(tmp_path)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Upload Test Failed: {str(e)}\")\n",
    "    print(f\"   Error Type: {type(e).__name__}\")\n",
    "    if hasattr(e, 'response'):\n",
    "        print(f\"   HTTP Status: {e.response.get('ResponseMetadata', {}).get('HTTPStatusCode')}\")\n",
    "        print(f\"   Error Code: {e.response.get('Error', {}).get('Code')}\")\n",
    "        print(f\"   Error Message: {e.response.get('Error', {}).get('Message')}\")\n",
    "    \n",
    "    # Clean up if file exists\n",
    "    if 'tmp_path' in locals() and os.path.exists(tmp_path):\n",
    "        os.unlink(tmp_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd202b3",
   "metadata": {},
   "source": [
    "## Test 4: Write to Cache Prefix (Actual Location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d02d301",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TEST 4: Write to Cache Prefix (Actual Location)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "try:\n",
    "    # Create a small test file\n",
    "    test_data = {\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "        'test': 'Cache prefix upload test',\n",
    "        'embedding': [0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "    }\n",
    "    \n",
    "    # Save to temp file (simulating the actual cache save)\n",
    "    with tempfile.NamedTemporaryFile(mode='wb', suffix='.pkl', delete=False) as tmp:\n",
    "        pickle.dump(test_data, tmp)\n",
    "        tmp_path = tmp.name\n",
    "    \n",
    "    # Generate cache key (similar to actual implementation)\n",
    "    cache_key = f\"{CACHE_PREFIX}/diagnostic_cache_test_{datetime.now().strftime('%Y%m%d_%H%M%S')}.pkl\"\n",
    "    \n",
    "    print(f\"üì§ Attempting upload to: s3://{S3_BUCKET}/{cache_key}\")\n",
    "    \n",
    "    # Upload to S3\n",
    "    s3.upload_file(\n",
    "        tmp_path,\n",
    "        S3_BUCKET,\n",
    "        cache_key\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ Cache upload successful!\")\n",
    "    \n",
    "    # Verify the upload\n",
    "    response = s3.head_object(Bucket=S3_BUCKET, Key=cache_key)\n",
    "    print(f\"‚úÖ File verified on S3\")\n",
    "    print(f\"   Size: {response['ContentLength']} bytes\")\n",
    "    print(f\"   ETag: {response['ETag']}\")\n",
    "    \n",
    "    # Clean up local file\n",
    "    os.unlink(tmp_path)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Cache Upload Test Failed: {str(e)}\")\n",
    "    print(f\"   Error Type: {type(e).__name__}\")\n",
    "    if hasattr(e, 'response'):\n",
    "        print(f\"   HTTP Status: {e.response.get('ResponseMetadata', {}).get('HTTPStatusCode')}\")\n",
    "        print(f\"   Error Code: {e.response.get('Error', {}).get('Code')}\")\n",
    "        print(f\"   Error Message: {e.response.get('Error', {}).get('Message')}\")\n",
    "    \n",
    "    # Clean up if file exists\n",
    "    if 'tmp_path' in locals() and os.path.exists(tmp_path):\n",
    "        os.unlink(tmp_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b551b5ff",
   "metadata": {},
   "source": [
    "## Test 5: Network Connectivity & Region Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3552f72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TEST 5: Network Connectivity & Region Check\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "try:\n",
    "    # Get bucket location\n",
    "    response = s3.get_bucket_location(Bucket=S3_BUCKET)\n",
    "    bucket_region = response['LocationConstraint'] or 'us-east-1'\n",
    "    print(f\"‚úÖ Bucket Region: {bucket_region}\")\n",
    "    \n",
    "    # Check if we're using the right region\n",
    "    client_region = s3.meta.region_name\n",
    "    print(f\"‚úÖ Client Region: {client_region}\")\n",
    "    \n",
    "    if bucket_region == client_region or (bucket_region == 'us-east-1' and client_region == 'us-east-1'):\n",
    "        print(\"‚úÖ Region configuration is correct\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è  Region mismatch! Bucket is in {bucket_region} but client is configured for {client_region}\")\n",
    "    \n",
    "    # Test basic connectivity\n",
    "    import socket\n",
    "    hostname = f\"s3.{bucket_region}.amazonaws.com\" if bucket_region != 'us-east-1' else \"s3.amazonaws.com\"\n",
    "    print(f\"\\nüåê Testing connectivity to {hostname}...\")\n",
    "    socket.create_connection((hostname, 443), timeout=5)\n",
    "    print(\"‚úÖ Network connectivity to S3 is working\")\n",
    "    \n",
    "except socket.timeout:\n",
    "    print(\"‚ùå Network timeout - unable to reach S3 endpoint\")\n",
    "except socket.error as e:\n",
    "    print(f\"‚ùå Network error: {str(e)}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Region Check Failed: {str(e)}\")\n",
    "    print(f\"   Error Type: {type(e).__name__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f702f3",
   "metadata": {},
   "source": [
    "## Test 6: IAM Permissions Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8cfcaaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TEST 6: IAM Permissions Summary\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "try:\n",
    "    # Try to get caller identity\n",
    "    sts = boto3.client(\n",
    "        'sts',\n",
    "        aws_access_key_id=AWS_ACCESS_KEY_ID,\n",
    "        aws_secret_access_key=AWS_SECRET_ACCESS_KEY,\n",
    "        region_name='us-east-1'\n",
    "    )\n",
    "    \n",
    "    identity = sts.get_caller_identity()\n",
    "    print(f\"‚úÖ AWS Account: {identity['Account']}\")\n",
    "    print(f\"‚úÖ User ARN: {identity['Arn']}\")\n",
    "    print(f\"‚úÖ User ID: {identity['UserId']}\")\n",
    "    \n",
    "    print(\"\\nüìã Required Permissions for this workflow:\")\n",
    "    permissions_needed = [\n",
    "        \"s3:ListBucket (on bucket)\",\n",
    "        \"s3:GetObject (on objects)\",\n",
    "        \"s3:PutObject (on objects)\",\n",
    "        \"s3:PutObjectAcl (for public-read ACL)\",\n",
    "        \"s3:DeleteObject (for cache cleanup)\"\n",
    "    ]\n",
    "    \n",
    "    for perm in permissions_needed:\n",
    "        print(f\"  ‚Ä¢ {perm}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  Could not retrieve IAM identity: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131e39e0",
   "metadata": {},
   "source": [
    "## üìä Diagnostic Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa171b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìä DIAGNOSTIC SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nIf all tests passed, the issue might be:\")\n",
    "print(\"  1. Race condition with many concurrent uploads\")\n",
    "print(\"  2. Temporary network instability\")\n",
    "print(\"  3. Rate limiting from too many rapid requests\")\n",
    "print(\"\\nIf Test 3 or 4 failed, the issue is likely:\")\n",
    "print(\"  1. Missing s3:PutObject permission\")\n",
    "print(\"  2. Bucket policy blocking uploads\")\n",
    "print(\"  3. Wrong bucket name or region\")\n",
    "print(\"\\nIf Test 1 failed:\")\n",
    "print(\"  1. Invalid AWS credentials\")\n",
    "print(\"  2. Credentials not properly set in Colab secrets\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
